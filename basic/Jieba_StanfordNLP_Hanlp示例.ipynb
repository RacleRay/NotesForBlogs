{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Jieba\" data-toc-modified-id=\"Jieba-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Jieba</a></span><ul class=\"toc-item\"><li><span><a href=\"#分词\" data-toc-modified-id=\"分词-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>分词</a></span></li><li><span><a href=\"#词性标注\" data-toc-modified-id=\"词性标注-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>词性标注</a></span></li></ul></li><li><span><a href=\"#Stanford-NLP\" data-toc-modified-id=\"Stanford-NLP-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Stanford NLP</a></span><ul class=\"toc-item\"><li><span><a href=\"#命名实体识别、词性标注\" data-toc-modified-id=\"命名实体识别、词性标注-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>命名实体识别、词性标注</a></span></li></ul></li><li><span><a href=\"#Hanlp\" data-toc-modified-id=\"Hanlp-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Hanlp</a></span><ul class=\"toc-item\"><li><span><a href=\"#HanLP分词\" data-toc-modified-id=\"HanLP分词-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>HanLP分词</a></span><ul class=\"toc-item\"><li><span><a href=\"#首次编译运行时，HanLP会自动构建词典缓存\" data-toc-modified-id=\"首次编译运行时，HanLP会自动构建词典缓存-3.1.1\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;</span>首次编译运行时，HanLP会自动构建词典缓存</a></span></li><li><span><a href=\"#CRF词法分析\" data-toc-modified-id=\"CRF词法分析-3.1.2\"><span class=\"toc-item-num\">3.1.2&nbsp;&nbsp;</span>CRF词法分析</a></span></li><li><span><a href=\"#NLP分词，更精准的中文分词、词性标注与命名实体识别\" data-toc-modified-id=\"NLP分词，更精准的中文分词、词性标注与命名实体识别-3.1.3\"><span class=\"toc-item-num\">3.1.3&nbsp;&nbsp;</span>NLP分词，更精准的中文分词、词性标注与命名实体识别</a></span></li><li><span><a href=\"#N最短路径分词\" data-toc-modified-id=\"N最短路径分词-3.1.4\"><span class=\"toc-item-num\">3.1.4&nbsp;&nbsp;</span>N最短路径分词</a></span></li><li><span><a href=\"#URL识别\" data-toc-modified-id=\"URL识别-3.1.5\"><span class=\"toc-item-num\">3.1.5&nbsp;&nbsp;</span>URL识别</a></span></li><li><span><a href=\"#基础分词\" data-toc-modified-id=\"基础分词-3.1.6\"><span class=\"toc-item-num\">3.1.6&nbsp;&nbsp;</span>基础分词</a></span></li><li><span><a href=\"#中国人名识别\" data-toc-modified-id=\"中国人名识别-3.1.7\"><span class=\"toc-item-num\">3.1.7&nbsp;&nbsp;</span>中国人名识别</a></span></li><li><span><a href=\"#用户词典的动态增删\" data-toc-modified-id=\"用户词典的动态增删-3.1.8\"><span class=\"toc-item-num\">3.1.8&nbsp;&nbsp;</span>用户词典的动态增删</a></span></li><li><span><a href=\"#演示自定义词性,以及往词典中插入自定义词性的词语\" data-toc-modified-id=\"演示自定义词性,以及往词典中插入自定义词性的词语-3.1.9\"><span class=\"toc-item-num\">3.1.9&nbsp;&nbsp;</span>演示自定义词性,以及往词典中插入自定义词性的词语</a></span></li><li><span><a href=\"#依存句法分析\" data-toc-modified-id=\"依存句法分析-3.1.10\"><span class=\"toc-item-num\">3.1.10&nbsp;&nbsp;</span>依存句法分析</a></span></li><li><span><a href=\"#极速分词\" data-toc-modified-id=\"极速分词-3.1.11\"><span class=\"toc-item-num\">3.1.11&nbsp;&nbsp;</span>极速分词</a></span></li><li><span><a href=\"#索引分词\" data-toc-modified-id=\"索引分词-3.1.12\"><span class=\"toc-item-num\">3.1.12&nbsp;&nbsp;</span>索引分词</a></span></li><li><span><a href=\"#日本人名识别\" data-toc-modified-id=\"日本人名识别-3.1.13\"><span class=\"toc-item-num\">3.1.13&nbsp;&nbsp;</span>日本人名识别</a></span></li><li><span><a href=\"#关键词提取TextRank\" data-toc-modified-id=\"关键词提取TextRank-3.1.14\"><span class=\"toc-item-num\">3.1.14&nbsp;&nbsp;</span>关键词提取TextRank</a></span></li><li><span><a href=\"#演示正规化字符\" data-toc-modified-id=\"演示正规化字符-3.1.15\"><span class=\"toc-item-num\">3.1.15&nbsp;&nbsp;</span>演示正规化字符</a></span></li><li><span><a href=\"#自动去除停用词、自动断句的分词器\" data-toc-modified-id=\"自动去除停用词、自动断句的分词器-3.1.16\"><span class=\"toc-item-num\">3.1.16&nbsp;&nbsp;</span>自动去除停用词、自动断句的分词器</a></span></li><li><span><a href=\"#数词和数量词识别\" data-toc-modified-id=\"数词和数量词识别-3.1.17\"><span class=\"toc-item-num\">3.1.17&nbsp;&nbsp;</span>数词和数量词识别</a></span></li><li><span><a href=\"#词共现统计\" data-toc-modified-id=\"词共现统计-3.1.18\"><span class=\"toc-item-num\">3.1.18&nbsp;&nbsp;</span>词共现统计</a></span></li><li><span><a href=\"#机构名识别\" data-toc-modified-id=\"机构名识别-3.1.19\"><span class=\"toc-item-num\">3.1.19&nbsp;&nbsp;</span>机构名识别</a></span></li><li><span><a href=\"#基于感知机序列标注的词法分析器\" data-toc-modified-id=\"基于感知机序列标注的词法分析器-3.1.20\"><span class=\"toc-item-num\">3.1.20&nbsp;&nbsp;</span>基于感知机序列标注的词法分析器</a></span></li><li><span><a href=\"#汉字转拼音\" data-toc-modified-id=\"汉字转拼音-3.1.21\"><span class=\"toc-item-num\">3.1.21&nbsp;&nbsp;</span>汉字转拼音</a></span></li><li><span><a href=\"#pinyin_to_chinese\" data-toc-modified-id=\"pinyin_to_chinese-3.1.22\"><span class=\"toc-item-num\">3.1.22&nbsp;&nbsp;</span>pinyin_to_chinese</a></span></li><li><span><a href=\"#地名识别\" data-toc-modified-id=\"地名识别-3.1.23\"><span class=\"toc-item-num\">3.1.23&nbsp;&nbsp;</span>地名识别</a></span></li><li><span><a href=\"#词性标注\" data-toc-modified-id=\"词性标注-3.1.24\"><span class=\"toc-item-num\">3.1.24&nbsp;&nbsp;</span>词性标注</a></span></li><li><span><a href=\"#同义改写\" data-toc-modified-id=\"同义改写-3.1.25\"><span class=\"toc-item-num\">3.1.25&nbsp;&nbsp;</span>同义改写</a></span></li><li><span><a href=\"#情感分析\" data-toc-modified-id=\"情感分析-3.1.26\"><span class=\"toc-item-num\">3.1.26&nbsp;&nbsp;</span>情感分析</a></span></li><li><span><a href=\"#添加stopword\" data-toc-modified-id=\"添加stopword-3.1.27\"><span class=\"toc-item-num\">3.1.27&nbsp;&nbsp;</span>添加stopword</a></span></li><li><span><a href=\"#文本推荐(句子级别\" data-toc-modified-id=\"文本推荐(句子级别-3.1.28\"><span class=\"toc-item-num\">3.1.28&nbsp;&nbsp;</span>文本推荐(句子级别</a></span></li><li><span><a href=\"#文本分类\" data-toc-modified-id=\"文本分类-3.1.29\"><span class=\"toc-item-num\">3.1.29&nbsp;&nbsp;</span>文本分类</a></span></li><li><span><a href=\"#文本聚类\" data-toc-modified-id=\"文本聚类-3.1.30\"><span class=\"toc-item-num\">3.1.30&nbsp;&nbsp;</span>文本聚类</a></span></li><li><span><a href=\"#动态设置预置分词器，全局\" data-toc-modified-id=\"动态设置预置分词器，全局-3.1.31\"><span class=\"toc-item-num\">3.1.31&nbsp;&nbsp;</span>动态设置预置分词器，全局</a></span></li><li><span><a href=\"#将简繁转换做到极致\" data-toc-modified-id=\"将简繁转换做到极致-3.1.32\"><span class=\"toc-item-num\">3.1.32&nbsp;&nbsp;</span>将简繁转换做到极致</a></span></li><li><span><a href=\"#音译人名识别\" data-toc-modified-id=\"音译人名识别-3.1.33\"><span class=\"toc-item-num\">3.1.33&nbsp;&nbsp;</span>音译人名识别</a></span></li><li><span><a href=\"#基于AhoCorasickDoubleArrayTrie的分词器，该分词器允许用户跳过核心词典，直接使用自己的词典\" data-toc-modified-id=\"基于AhoCorasickDoubleArrayTrie的分词器，该分词器允许用户跳过核心词典，直接使用自己的词典-3.1.34\"><span class=\"toc-item-num\">3.1.34&nbsp;&nbsp;</span>基于AhoCorasickDoubleArrayTrie的分词器，该分词器允许用户跳过核心词典，直接使用自己的词典</a></span></li><li><span><a href=\"#语义距离\" data-toc-modified-id=\"语义距离-3.1.35\"><span class=\"toc-item-num\">3.1.35&nbsp;&nbsp;</span>语义距离</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache C:\\Users\\qw\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.301 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Mode: 我/ 来到/ 北京/ 清华/ 清华大学/ 华大/ 大学\n"
     ]
    }
   ],
   "source": [
    "seg_list = jieba.cut(\"我来到北京清华大学\", cut_all=True)\n",
    "print(\"Full Mode: \" + \"/ \".join(seg_list))  # 全模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "??jieba.cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default Mode: 我/ 来到/ 北京/ 清华大学\n"
     ]
    }
   ],
   "source": [
    "seg_list = jieba.cut(\"我来到北京清华大学\", cut_all=False)\n",
    "print(\"Default Mode: \" + \"/ \".join(seg_list))  # 精确模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "他, 来到, 了, 网易, 杭研, 大厦\n"
     ]
    }
   ],
   "source": [
    "seg_list = jieba.cut(\"他来到了网易杭研大厦\")  # 默认是精确模式\n",
    "print(\", \".join(seg_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "小明, 硕士, 毕业, 于, 中国, 科学, 学院, 科学院, 中国科学院, 计算, 计算所, ，, 后, 在, 日本, 京都, 大学, 日本京都大学, 深造\n"
     ]
    }
   ],
   "source": [
    "# 搜索引擎模式\n",
    "seg_list = jieba.cut_for_search(\"小明硕士毕业于中国科学院计算所，后在日本京都大学深造\")  \n",
    "print(\", \".join(seg_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词性标注"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "是 v\n",
      "广泛 a\n",
      "使用 v\n",
      "的 uj\n",
      "中文 nz\n",
      "分词 n\n",
      "工具 n\n",
      "， x\n",
      "具有 v\n",
      "以下 f\n",
      "特点 n\n",
      "： x\n"
     ]
    }
   ],
   "source": [
    "import jieba.posseg as pseg \n",
    "\n",
    "strings=\"是广泛使用的中文分词工具，具有以下特点：\"\n",
    "words = pseg.cut(strings)\n",
    "\n",
    "for word, flag in words:\n",
    "    print('%s %s' % (word, flag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stanford NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 命名实体识别、词性标注"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "\n",
    "# 输入stanford nlp文件存放路径\n",
    "nlp = StanfordCoreNLP(r'D:\\ProgramData\\nlp_package\\stanford-corenlp-full-2018-10-05', \n",
    "                      lang='zh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin=open('1-data/news.txt','r',encoding='utf8')\n",
    "fner=open('1-data/ner.txt','w',encoding='utf8')\n",
    "ftag=open('1-data/pos_tag.txt','w',encoding='utf8')\n",
    "\n",
    "for line in fin:\n",
    "    line=line.strip()\n",
    "    if len(line) < 1:\n",
    "        continue\n",
    "        \n",
    "    fner.write(\" \".join([item[0]+'/'+item[1] for item in nlp.ner(line) if len(item)==2 ])+\"\\n\")\n",
    "    ftag.write(\" \".join([item[0]+\"/\"+item[1] for item in nlp.pos_tag(line) if len(item)==2 ])+\"\\n\")\n",
    "    \n",
    "fin.close()\n",
    "fner.close()   \n",
    "ftag.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hanlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from jpype import *\n",
    "\n",
    "# # 输入hanlp文件存放路径\n",
    "# startJVM(getDefaultJVMPath(), \"-Djava.class.path=D:\\ProgramData\\nlp_package\\hanlp-1.7.0-release\\hanlp-1.7.0.jar;D:\\ProgramData\\nlp_package\\hanlp-1.7.0-release\",\n",
    "#          \"-Xms1g\",\n",
    "#          \"-Xmx1g\") # 启动JVM，Linux需替换分号;为冒号:\n",
    "# \n",
    "# # 初始化java类，在python中使用\n",
    "# HanLP = JClass('com.hankcs.hanlp.HanLP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\\\'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.sep # 根据系统生成文件路径分隔符"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HanLP分词\n",
    "\n",
    "https://github.com/hankcs/HanLP/blob/master/data/dictionary/other/TagPKU98.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhanlp import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================HanLP分词==============================\n",
      "[你好/vl, ，/w, 欢迎/v, 在/p, Python/nx, 中/f, 调用/v, HanLP/nx, 的/ude1, API/nx]\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 30 + \"HanLP分词\" + \"=\" * 30)\n",
    "print(HanLP.segment('你好，欢迎在Python中调用HanLP的API'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================标准分词==============================\n",
      "[你好/vl, ，/w, 欢迎/v, 在/p, Python/nx, 中/f, 调用/v, HanLP/nx, 的/ude1, API/nx]\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 30 + \"标准分词\" + \"=\" * 30)\n",
    "StandardTokenizer = JClass('com.hankcs.hanlp.tokenizer.StandardTokenizer')\n",
    "print(StandardTokenizer.segment('你好，欢迎在Python中调用HanLP的API'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================NLP分词==============================\n",
      "[中国科学院计算技术研究所/n, 的/u, 宗/n, 成庆/nr, 教授/n, 正在/d, 教授/n, 自然语言处理/v, 课程/n]\n"
     ]
    }
   ],
   "source": [
    "# NLP分词NLPTokenizer会执行全部命名实体识别和词性标注\n",
    "print(\"=\" * 30 + \"NLP分词\" + \"=\" * 30)\n",
    "NLPTokenizer = JClass('com.hankcs.hanlp.tokenizer.NLPTokenizer')\n",
    "print(NLPTokenizer.segment('中国科学院计算技术研究所的宗成庆教授正在教授自然语言处理课程'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我/代词 的/助词 希望/名词 是/动词 希望/动词 张晚霞/人名 的/助词 背影/名词 被/介词 晚霞/名词 映红/人名\n",
      "支援/v 臺灣/ns 正體/n 香港/ns 繁體/n ：/v 微软公司/ntc 於/p 1975年/t 由/p 比爾·蓋茲/n 和/c 保羅·艾倫/nr 創立/v 。/w\n"
     ]
    }
   ],
   "source": [
    "# .translateLabels()\n",
    "print(NLPTokenizer.analyze(\"我的希望是希望张晚霞的背影被晚霞映红\").translateLabels())\n",
    "print(NLPTokenizer.analyze(\"支援臺灣正體香港繁體：微软公司於1975年由比爾·蓋茲和保羅·艾倫創立。\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== 极速词典分词==============================\n",
      "[江西/ns, 鄱阳湖/ns, 干枯/v, ，/w, 中国/ns, 最大/d, 淡水湖/Vg, 变成/v, 大/a, 草原/n]\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 30 + \" 极速词典分词\" + \"=\" * 30)\n",
    "SpeedTokenizer = JClass('com.hankcs.hanlp.tokenizer.SpeedTokenizer')\n",
    "print(NLPTokenizer.segment('江西鄱阳湖干枯，中国最大淡水湖变成大草原'))\n",
    "print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================索引分词==============================\n",
      "主副食品/n [0:4]\n",
      "主副食/j [0:3]\n",
      "副食品/n [1:4]\n",
      "副食/n [1:3]\n",
      "食品/n [2:4]\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 30 + \"索引分词\" + \"=\" * 30)\n",
    "IndexTokenizer = JClass('com.hankcs.hanlp.tokenizer.IndexTokenizer')\n",
    "termList = IndexTokenizer.segment(\"主副食品\");\n",
    "for term in termList:\n",
    "    print(str(term) + \" [\" + str(term.offset) + \":\" + str(term.offset + len(term.word)) + \"]\")\n",
    "print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== 自定义分词==============================\n",
      "[攻城狮/nz, 逆袭/nz, 单身狗/nz, ，/w, 迎娶/v, 白富美/nr, ，/w, 走上/v, 人生/n, 巅峰/n]\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 30 + \" 自定义分词\" + \"=\" * 30)\n",
    "CustomDictionary = JClass('com.hankcs.hanlp.dictionary.CustomDictionary')\n",
    "CustomDictionary.add('攻城狮')\n",
    "CustomDictionary.add('单身狗')\n",
    "HanLP = JClass('com.hankcs.hanlp.HanLP')\n",
    "print(HanLP.segment('攻城狮逆袭单身狗，迎娶白富美，走上人生巅峰'))\n",
    "print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[水资源, 陈明忠]\n"
     ]
    }
   ],
   "source": [
    "# 关键词提取\n",
    "document = \"水利部水资源司司长陈明忠9月29日在国务院新闻办举行的新闻发布会上透露，\" \\\n",
    "           \"根据刚刚完成了水资源管理制度的考核，有部分省接近了红线的指标，\" \\\n",
    "           \"有部分省超过红线的指标。对一些超过红线的地方，陈明忠表示，对一些取用水项目进行区域的限批，\" \\\n",
    "           \"严格地进行水资源论证和取水许可的批准。\"\n",
    "print(HanLP.extractKeyword(document, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[严格地进行水资源论证和取水许可的批准, 水利部水资源司司长陈明忠9月29日在国务院新闻办举行的新闻发布会上透露, 有部分省超过红线的指标]\n"
     ]
    }
   ],
   "source": [
    "# 自动摘要\n",
    "print(HanLP.extractSummary(document, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\t徐先生\t徐先生\tnh\tnr\t_\t4\t主谓关系\t_\t_\n",
      "2\t还\t还\td\td\t_\t4\t状中结构\t_\t_\n",
      "3\t具体\t具体\ta\tad\t_\t4\t状中结构\t_\t_\n",
      "4\t帮助\t帮助\tv\tv\t_\t0\t核心关系\t_\t_\n",
      "5\t他\t他\tr\tr\t_\t4\t兼语\t_\t_\n",
      "6\t确定\t确定\tv\tv\t_\t4\t动宾关系\t_\t_\n",
      "7\t了\t了\tu\tu\t_\t6\t右附加关系\t_\t_\n",
      "8\t把\t把\tp\tp\t_\t15\t状中结构\t_\t_\n",
      "9\t画\t画\tv\tv\t_\t8\t介宾关系\t_\t_\n",
      "10\t雄鹰\t雄鹰\tn\tn\t_\t9\t动宾关系\t_\t_\n",
      "11\t、\t、\twp\tw\t_\t12\t标点符号\t_\t_\n",
      "12\t松鼠\t松鼠\tn\tn\t_\t10\t并列关系\t_\t_\n",
      "13\t和\t和\tc\tc\t_\t14\t左附加关系\t_\t_\n",
      "14\t麻雀\t麻雀\tn\tn\t_\t10\t并列关系\t_\t_\n",
      "15\t作为\t作为\tv\tv\t_\t6\t动宾关系\t_\t_\n",
      "16\t主攻\t主攻\tv\tvn\t_\t17\t定中关系\t_\t_\n",
      "17\t目标\t目标\tn\tn\t_\t15\t动宾关系\t_\t_\n",
      "18\t。\t。\twp\tw\t_\t4\t标点符号\t_\t_\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 依存句法分析\n",
    "print(HanLP.parseDependency(\"徐先生还具体帮助他确定了把画雄鹰、松鼠和麻雀作为主攻目标。\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================短语提取==============================\n",
      "[算法工程师, 算法处理, 一维信息, 算法研究, 信号处理, 信息算法, 通信物理层, 处理算法, 视频算法, 互联网搜索算法]\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "text = r\"算法工程师\\n 算法（Algorithm）是一系列解决问题的清晰指令，也就是说，能够对一定规范的输入，在有限时间内获得所要求的输出。如果一个算法有缺陷，或不适合于某个问题，执行这个算法将不会解决这个问题。不同的算法可能用不同的时间、空间或效率来完成同样的任务。一个算法的优劣可以用空间复杂度与时间复杂度来衡量。算法工程师就是利用算法处理事物的人。\\n \\n 1职位简介\\n 算法工程师是一个非常高端的职位；\\n 专业要求：计算机、电子、通信、数学等相关专业；\\n 学历要求：本科及其以上的学历，大多数是硕士学历及其以上；\\n 语言要求：英语要求是熟练，基本上能阅读国外专业书刊；\\n 必须掌握计算机相关知识，熟练使用仿真工具MATLAB等，必须会一门编程语言。\\n\\n2研究方向\\n 视频算法工程师、图像处理算法工程师、音频算法工程师 通信基带算法工程师\\n \\n 3目前国内外状况\\n 目前国内从事算法研究的工程师不少，但是高级算法工程师却很少，是一个非常紧缺的专业工程师。算法工程师根据研究领域来分主要有音频/视频算法处理、图像技术方面的二维信息算法处理和通信物理层、雷达信号处理、生物医学信号处理等领域的一维信息算法处理。\\n 在计算机音视频和图形图像技术等二维信息算法处理方面目前比较先进的视频处理算法：机器视觉成为此类算法研究的核心；另外还有2D转3D算法(2D-to-3D conversion)，去隔行算法(de-interlacing)，运动估计运动补偿算法(Motion estimation/Motion Compensation)，去噪算法(Noise Reduction)，缩放算法(scaling)，锐化处理算法(Sharpness)，超分辨率算法(Super Resolution),手势识别(gesture recognition),人脸识别(face recognition)。\\n 在通信物理层等一维信息领域目前常用的算法：无线领域的RRM、RTT，传送领域的调制解调、信道均衡、信号检测、网络优化、信号分解等。\\n 另外数据挖掘、互联网搜索算法也成为当今的热门方向。\\n\"\n",
    "print(\"=\" * 30 + \"短语提取\" + \"=\" * 30)\n",
    "print(HanLP.extractPhrase(text, 10))\n",
    "print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 首次编译运行时，HanLP会自动构建词典缓存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_at_first_sight():\n",
    "    \"\"\"第一个Demo，惊鸿一瞥\n",
    "    >>> demo_at_first_sight()\n",
    "    首次编译运行时，HanLP会自动构建词典缓存，请稍候……\n",
    "    [你好/vl, ，/w, 欢迎/v, 使用/v, HanLP/nx, 汉语/gi, 处理/vn, 包/v, ！/w, 接下来/vl, 请/v, 从/p, 其他/rzv, Demo/nx, 中/f, 体验/v, HanLP/nx, 丰富/a, 的/ude1, 功能/n, ~/nx]\n",
    "    \"\"\"\n",
    "    print(\"首次编译运行时，HanLP会自动构建词典缓存，请稍候……\")\n",
    "    #HanLP.Config.enableDebug()\n",
    "    print(HanLP.segment(\n",
    "            \"你好，欢迎使用HanLP汉语处理包！接下来请从其他Demo中体验HanLP丰富的功能~\"))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import doctest\n",
    "    doctest.testmod(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CRF词法分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_CRF_lexical_analyzer(tests):\n",
    "    \"\"\" CRF词法分析器\n",
    "    >>> tests = [\"商品和服务\",\n",
    "    ... \"上海华安工业（集团）公司董事长谭旭光和秘书胡花蕊来到美国纽约现代艺术博物馆参观\",\n",
    "    ... \"微软公司於1975年由比爾·蓋茲和保羅·艾倫創立，18年啟動以智慧雲端、前端為導向的大改組。\"]\n",
    "    >>> demo_CRF_lexical_analyzer(tests)\n",
    "    商品/n 和/c 服务/vn\n",
    "    [上海/ns 华安/nz 工业/n （/n 集团/n ）/v 公司/n]/nt 董事长/n 谭旭光/nr 和/c 秘书/n\n",
    "        胡花蕊/nr 来到/v [美国/ns 纽约/ns 现代/t 艺术/n 博物馆/n]/ns 参观/v\n",
    "    [微软/nt 公司/n]/nt 於/p 1975年/t 由/p 比爾·蓋茲/n 和/c 保羅·艾倫/v 創立/v ，/v\n",
    "        18年/n 啟動/v 以/p 智慧/n 雲端/n 、/v 前端/n 為/v 導向/n 的/u 大/a 改組/vn 。/w\n",
    "    \"\"\"\n",
    "    CRFLexicalAnalyzer = JClass(\"com.hankcs.hanlp.model.crf.CRFLexicalAnalyzer\")\n",
    "    analyzer = CRFLexicalAnalyzer()\n",
    "    for sentence in tests:\n",
    "        print(analyzer.analyze(sentence))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import doctest\n",
    "    # Test examples in docstrings in functions and classes reachable\n",
    "    # from module m (or the current module if m is not supplied), starting\n",
    "    # with m.__doc__.\n",
    "    doctest.testmod(verbose=True, optionflags=doctest.NORMALIZE_WHITESPACE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLP分词，更精准的中文分词、词性标注与命名实体识别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_NLP_segment():\n",
    "    \"\"\" NLP分词，更精准的中文分词、词性标注与命名实体识别\n",
    "        标注集请查阅 https://github.com/hankcs/HanLP/blob/master/data/dictionary/other/TagPKU98.csv\n",
    "        或者干脆调用 Sentence#translateLabels() 转为中文\n",
    "    >>> demo_NLP_segment()\n",
    "    [我/r, 新造/v, 一个/m, 词/n, 叫/v, 幻想乡/ns, 你/r, 能/v, 识别/v, 并/c, 正确/ad, 标注/v, 词性/n, 吗/y, ？/w]\n",
    "    我/代词 的/助词 希望/名词 是/动词 希望/动词 张晚霞/人名 的/助词 背影/名词 被/介词 晚霞/名词 映/动词 红/形容词\n",
    "    支援/v 臺灣/ns 正體/n 香港/ns 繁體/n ：/w [微软/nt 公司/n]/nt 於/p 1975年/t 由/p 比爾·蓋茲/n 和/c 保羅·艾倫/nr 創立/v 。/w\n",
    "    \"\"\"\n",
    "    NLPTokenizer = JClass(\"com.hankcs.hanlp.tokenizer.NLPTokenizer\")\n",
    "    print(NLPTokenizer.segment(\"我新造一个词叫幻想乡你能识别并正确标注词性吗？\"))  # “正确”是副形词。\n",
    "    # 注意观察下面两个“希望”的词性、两个“晚霞”的词性\n",
    "    print(NLPTokenizer.analyze(\"我的希望是希望张晚霞的背影被晚霞映红\").translateLabels())\n",
    "    print(NLPTokenizer.analyze(\"支援臺灣正體香港繁體：微软公司於1975年由比爾·蓋茲和保羅·艾倫創立。\"))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import doctest\n",
    "    doctest.testmod(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  N最短路径分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_NShort_segment(sentences):\n",
    "    \"\"\" N最短路径分词，该分词器比最短路分词器慢，但是效果稍微好一些，对命名实体识别能力更强\n",
    "    >>> sentences = [\n",
    "    ...    \"今天，刘志军案的关键人物,山西女商人丁书苗在市二中院出庭受审。\",\n",
    "    ...    \"江西省监狱管理局与中国太平洋财产保险股份有限公司南昌中心支公司保险合同纠纷案\",\n",
    "    ...    \"新北商贸有限公司\",\n",
    "    ...    ]\n",
    "    >>> demo_NShort_segment(sentences)\n",
    "    N-最短分词：[今天/t, ，/w, 刘志军/nr, 案/ng, 的/ude1, 关键/n, 人物/n, ,/w, 山西/ns,\n",
    "        女/b, 商人/nnt, 丁书苗/nr, 在/p, 市/n, 二/m, 中/f, 院/n, 出庭/vi, 受审/vi, 。/w]\n",
    "    最短路分词：[今天/t, ，/w, 刘志军/nr, 案/ng, 的/ude1, 关键/n, 人物/n, ,/w, 山西/ns,\n",
    "        女/b, 商人/nnt, 丁书苗/nr, 在/p, 市/n, 二/m, 中/f, 院/n, 出庭/vi, 受审/vi, 。/w]\n",
    "    N-最短分词：[江西省监狱管理局/nt, 与/cc, 中国/ns, 太平洋/ns, 财产/n, 保险/n, 股份/n,\n",
    "        有限公司/nis, 南昌/ns, 中心支公司/nt, 保险/n, 合同/n, 纠纷案/nz]\n",
    "    最短路分词：[江西省监狱管理局/nt, 与/cc, 中国/ns, 太平洋/ns, 财产/n, 保险/n, 股份/n,\n",
    "        有限公司/nis, 南昌/ns, 中心支公司/nt, 保险/n, 合同/n, 纠纷案/nz]\n",
    "    N-最短分词：[新北商贸有限公司/nt]\n",
    "    最短路分词：[新北商贸有限公司/nt]\n",
    "    \"\"\"\n",
    "    NShortSegment = JClass(\"com.hankcs.hanlp.seg.NShort.NShortSegment\")\n",
    "    Segment = JClass(\"com.hankcs.hanlp.seg.Segment\")\n",
    "    ViterbiSegment = JClass(\"com.hankcs.hanlp.seg.Viterbi.ViterbiSegment\")\n",
    "\n",
    "    nshort_segment = NShortSegment().enableCustomDictionary(False).enablePlaceRecognize(\n",
    "        True).enableOrganizationRecognize(True)\n",
    "    shortest_segment = ViterbiSegment().enableCustomDictionary(\n",
    "        False).enablePlaceRecognize(True).enableOrganizationRecognize(True)\n",
    "\n",
    "    for sentence in sentences:\n",
    "        print(\"N-最短分词：{} \\n最短路分词：{}\".format(\n",
    "            nshort_segment.seg(sentence), shortest_segment.seg(sentence)))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import doctest\n",
    "    doctest.testmod(verbose=True, optionflags=doctest.NORMALIZE_WHITESPACE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### URL识别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_URL_recognition(text):\n",
    "    \"\"\" 演示URL识别\n",
    "    >>> text = '''HanLP的项目地址是https://github.com/hankcs/HanLP，\n",
    "    ... 发布地址是https://github.com/hankcs/HanLP/releases，\n",
    "    ... 我有时候会在www.hankcs.com上面发布一些消息，\n",
    "    ... 我的微博是http://weibo.com/hankcs/，会同步推送hankcs.com的新闻。\n",
    "    ... 听说.中国域名开放申请了,但我并没有申请hankcs.中国,因为穷……\n",
    "    ... '''\n",
    "    >>> demo_URL_recognition(text)\n",
    "    [HanLP/nx, 的/ude1, 项目/n, 地址/n, 是/vshi, https://github.com/hankcs/HanLP/xu, ，/w,\n",
    "    /w, 发布/v, 地址/n, 是/vshi, https://github.com/hankcs/HanLP/releases/xu, ，/w,\n",
    "    /w, 我/rr, 有时候/d, 会/v, 在/p, www.hankcs.com/xu, 上面/f, 发布/v, 一些/m, 消息/n, ，/w,\n",
    "    /w, 我/rr, 的/ude1, 微博/n, 是/vshi, http://weibo.com/hankcs//xu, ，/w, 会/v,\n",
    "        同步/vd, 推送/nz, hankcs.com/xu, 的/ude1, 新闻/n, 。/w,\n",
    "    /w, 听说/v, ./w, 中国/ns, 域名/n, 开放/v, 申请/v, 了/ule, ,/w, 但/c, 我/rr, 并/cc,\n",
    "        没有/v, 申请/v, hankcs.中国/xu, ,/w, 因为/c, 穷/a, ……/w,\n",
    "    /w]\n",
    "    https://github.com/hankcs/HanLP\n",
    "    https://github.com/hankcs/HanLP/releases\n",
    "    www.hankcs.com\n",
    "    http://weibo.com/hankcs/\n",
    "    hankcs.com\n",
    "    hankcs.中国\n",
    "    \"\"\"\n",
    "    Nature = JClass(\"com.hankcs.hanlp.corpus.tag.Nature\")\n",
    "    Term = JClass(\"com.hankcs.hanlp.seg.common.Term\")\n",
    "    URLTokenizer = JClass(\"com.hankcs.hanlp.tokenizer.URLTokenizer\")\n",
    "\n",
    "    term_list = URLTokenizer.segment(text)\n",
    "    print(term_list)\n",
    "    \n",
    "    # 输出识别url\n",
    "    for term in term_list:\n",
    "        if term.nature == Nature.xu:\n",
    "            print(term.word)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import doctest\n",
    "    doctest.testmod(verbose=True, optionflags=doctest.NORMALIZE_WHITESPACE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 基础分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def demo_basic_tokenizer(text):\n",
    "    \"\"\" 演示基础分词，基础分词只进行基本NGram分词，不识别命名实体，不使用用户词典\n",
    "    >>> text = (\"举办纪念活动铭记二战历史，不忘战争带给人类的深重灾难，是为了防止悲剧重演，确保和平永驻；\"\n",
    "    ...    \"铭记二战历史，更是为了提醒国际社会，需要共同捍卫二战胜利成果和国际公平正义，\"\n",
    "    ...    \"必须警惕和抵制在历史认知和维护战后国际秩序问题上的倒行逆施。\")\n",
    "    >>> demo_basic_tokenizer(text)\n",
    "    [举办/v, 纪念活动/nz, 铭记/v, 二战/n, 历史/n, ，/w, 不忘/v, 战争/n, 带给/v, 人类/n,\n",
    "     的/ude1, 深重/a, 灾难/n, ，/w, 是/vshi, 为了/p, 防止/v, 悲剧/n, 重演/v, ，/w, 确保/v,\n",
    "     和平/n, 永驻/nz, ；/w, 铭记/v, 二战/n, 历史/n, ，/w, 更是/d, 为了/p, 提醒/v, 国际/n,\n",
    "      社会/n, ，/w, 需要/v, 共同/d, 捍卫/v, 二战/n, 胜利/vn, 成果/n, 和/cc, 国际/n, 公平/a,\n",
    "       正义/n, ，/w, 必须/d, 警惕/v, 和/cc, 抵制/v, 在/p, 历史/n, 认知/vn, 和/cc, 维护/v,\n",
    "       战后/t, 国际/n, 秩序/n, 问题/n, 上/f, 的/ude1, 倒行逆施/vl, 。/w]\n",
    "    BasicTokenizer分词速度：1499051.04字每秒\n",
    "    \"\"\"\n",
    "    BasicTokenizer = JClass(\"com.hankcs.hanlp.tokenizer.BasicTokenizer\")\n",
    "    print(BasicTokenizer.segment(text))\n",
    "\n",
    "    start = time.time()\n",
    "    pressure = 100000\n",
    "    for i in range(pressure):\n",
    "        BasicTokenizer.segment(text)\n",
    "    cost_time = time.time() - start\n",
    "    print(\"BasicTokenizer分词速度：%.2f字每秒\" % (\n",
    "        len(text) * pressure / cost_time))\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    import doctest\n",
    "    # doctest.NORMALIZE_WHITESPACE 输出比对的文档，忽略换行等空格。防止一行过长\n",
    "    doctest.testmod(verbose=True, optionflags=doctest.NORMALIZE_WHITESPACE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 中国人名识别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_chinese_name_recognition(sentences):\n",
    "    \"\"\" 中国人名识别\n",
    "    >>> sentences = [\n",
    "    ...    \"签约仪式前，秦光荣、李纪恒、仇和等一同会见了参加签约的企业家。\",\n",
    "    ...    \"武大靖创世界纪录夺冠，中国代表团平昌首金\",\n",
    "    ...    \"区长庄木弟新年致辞\",\n",
    "    ...    \"朱立伦：两岸都希望共创双赢 习朱历史会晤在即\",\n",
    "    ...    \"陕西首富吴一坚被带走 与令计划妻子有交集\",\n",
    "    ...    \"据美国之音电台网站4月28日报道，8岁的凯瑟琳·克罗尔（凤甫娟）和很多华裔美国小朋友一样，小小年纪就开始学小提琴了。她的妈妈是位虎妈么？\",\n",
    "    ...    \"凯瑟琳和露西（庐瑞媛），跟她们的哥哥们有一些不同。\",\n",
    "    ...    \"王国强、高峰、汪洋、张朝阳光着头、韩寒、小四\",\n",
    "    ...    \"张浩和胡健康复员回家了\",\n",
    "    ...    \"王总和小丽结婚了\",\n",
    "    ...    \"编剧邵钧林和稽道青说\",\n",
    "    ...    \"这里有关天培的有关事迹\",\n",
    "    ...    \"龚学平等领导说,邓颖超生前杜绝超生\",]\n",
    "    >>> demo_chinese_name_recognition(sentences)\n",
    "    [签约/vi, 仪式/n, 前/f, ，/w, 秦光荣/nr, 、/w, 李纪恒/nr, 、/w, 仇和/nr, 等/udeng, 一同/d, 会见/v, 了/ule, 参加/v, 签约/vi, 的/ude1, 企业家/nnt, 。/w]\n",
    "    [武大靖/nr, 创/v, 世界纪录/nz, 夺冠/vi, ，/w, 中国代表团/nt, 平昌/ns, 首金/n]\n",
    "    [区长/nnt, 庄木弟/nr, 新年/t, 致辞/vi]\n",
    "    [朱立伦/nr, ：/w, 两岸/n, 都/d, 希望/v, 共创/v, 双赢/n,  /w, 习/v, 朱/ag, 历史/n, 会晤/vn, 在即/vi]\n",
    "    [陕西/ns, 首富/n, 吴一坚/nr, 被/pbei, 带走/v,  /w, 与/cc, 令计划/nr, 妻子/n, 有/vyou, 交集/v]\n",
    "    [据/p, 美国之音/n, 电台/nis, 网站/n, 4/m, 月/n, 28/m, 日/b, 报道/v, ，/w, 8/m, 岁/qt, 的/ude1, 凯瑟琳·克罗尔/nrf, （/w, 凤甫娟/nr, ）/w, 和/cc, 很多/m, 华裔/n, 美国/nsf, 小朋友/n, 一样/uyy, ，/w, 小小年纪/n, 就/d, 开始/v, 学/v, 小提琴/n, 了/ule, 。/w, 她/rr, 的/ude1, 妈妈/n, 是/vshi, 位/q, 虎妈/nz, 么/y, ？/w]\n",
    "    [凯瑟琳/nrf, 和/cc, 露西/nrf, （/w, 庐瑞媛/nr, ）/w, ，/w, 跟/p, 她们/rr, 的/ude1, 哥哥/n, 们/k, 有/vyou, 一些/m, 不同/a, 。/w]\n",
    "    [王国强/nr, 、/w, 高峰/n, 、/w, 汪洋/n, 、/w, 张朝阳/nr, 光着头/l, 、/w, 韩寒/nr, 、/w, 小/a, 四/m]\n",
    "    [张浩/nr, 和/cc, 胡健康/nr, 复员/v, 回家/vi, 了/ule]\n",
    "    [王总/nr, 和/cc, 小丽/nr, 结婚/vi, 了/ule]\n",
    "    [编剧/nnt, 邵钧林/nr, 和/cc, 稽道青/nr, 说/v]\n",
    "    [这里/rzs, 有/vyou, 关天培/nr, 的/ude1, 有关/vn, 事迹/n]\n",
    "    [龚学平/nr, 等/udeng, 领导/n, 说/v, ,/w, 邓颖超/nr, 生前/t, 杜绝/v, 超生/vi]\n",
    "    \"\"\"\n",
    "    segment = HanLP.newSegment().enableNameRecognize(True);\n",
    "    for sentence in sentences:\n",
    "        term_list = segment.seg(sentence)\n",
    "        print(term_list)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import doctest\n",
    "    doctest.testmod(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 用户词典的动态增删"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_custom_dictionary(text):\n",
    "    \"\"\" 演示用户词典的动态增删\n",
    "    TO-DO:\n",
    "    DoubleArrayTrie分词\n",
    "    首字哈希之后二分的trie树分词\n",
    "    >>> text = \"攻城狮逆袭单身狗，迎娶白富美，走上人生巅峰\"  # 怎么可能噗哈哈！\n",
    "    >>> demo_custom_dictionary(text)\n",
    "    [攻城/vi, 狮/ng, 逆袭/nz, 单身/n, 狗/n, ，/w, 迎娶/v, 白富美/nr, ，/w, 走上/v, 人生/n, 巅峰/n]\n",
    "    [攻城狮/nz, 逆袭/nz, 单身狗/nz, ，/w, 迎娶/v, 白富美/nz, ，/w, 走上/v, 人生/n, 巅峰/n]\n",
    "    \"\"\"\n",
    "    print(HanLP.segment(text))\n",
    "\n",
    "    CustomDictionary = JClass(\"com.hankcs.hanlp.dictionary.CustomDictionary\")\n",
    "    CustomDictionary.add(\"攻城狮\")  # 动态增加\n",
    "    CustomDictionary.insert(\"白富美\", \"nz 1024\")  # 强行插入\n",
    "    #CustomDictionary.remove(\"攻城狮\"); # 删除词语（注释掉试试）\n",
    "    CustomDictionary.add(\"单身狗\", \"nz 1024 n 1\")\n",
    "    #print(CustomDictionary.get(\"单身狗\"))\n",
    "\n",
    "    print(HanLP.segment(text))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import doctest\n",
    "    doctest.testmod(verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 演示自定义词性,以及往词典中插入自定义词性的词语"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_custom_nature():\n",
    "    \"\"\" 演示自定义词性,以及往词典中插入自定义词性的词语\n",
    "        !!!由于采用了反射技术,用户需对本地环境的兼容性和稳定性负责!!!\n",
    "    TO-DO\n",
    "    如果使用了动态词性之后任何类使用了switch(nature)语句,必须注册每个类\n",
    "    >>> demo_custom_nature()\n",
    "    n\n",
    "    None\n",
    "    电脑品牌\n",
    "    [苹果电脑/电脑品牌, 可以/v, 运行/vn, 开源/v, 阿尔法/nrf, 狗/n, 代码/n, 吗/y]\n",
    "    找到了 [电脑品牌] : 苹果电脑\n",
    "    <BLANKLINE>\n",
    "    [苹果电脑/电脑品牌, 可以/v, 运行/vn, 开源/v, 阿尔法狗/科技名词, 代码/n, 吗/y]\n",
    "    \"\"\"\n",
    "    # 对于系统中已有的词性,可以直接获取\n",
    "    Nature = JClass(\"com.hankcs.hanlp.corpus.tag.Nature\")\n",
    "    pc_nature = Nature.fromString(\"n\")\n",
    "    print(pc_nature)\n",
    "    \n",
    "    # 此时系统中没有\"电脑品牌\"这个词性\n",
    "    pc_nature = Nature.fromString(\"电脑品牌\")\n",
    "    print(pc_nature)\n",
    "    \n",
    "    # 我们可以动态添加一个\n",
    "    pc_nature = Nature.create(\"电脑品牌\");\n",
    "    print(pc_nature)\n",
    "    \n",
    "    # 可以将它赋予到某个词语\n",
    "    LexiconUtility = JClass(\"com.hankcs.hanlp.utility.LexiconUtility\")\n",
    "    LexiconUtility.setAttribute(\"苹果电脑\", pc_nature)\n",
    "    \n",
    "    # 或者\n",
    "    LexiconUtility.setAttribute(\"苹果电脑\", \"电脑品牌 1000\")\n",
    "    \n",
    "    # 它们将在分词结果中生效\n",
    "    term_list = HanLP.segment(\"苹果电脑可以运行开源阿尔法狗代码吗\")\n",
    "    print(term_list)\n",
    "    \n",
    "    for term in term_list:\n",
    "        if term.nature == pc_nature:\n",
    "            print(\"找到了 [{}] : {}\\n\".format(pc_nature, term.word))\n",
    "\n",
    "    # 还可以直接插入到用户词典\n",
    "    CustomDictionary = JClass(\"com.hankcs.hanlp.dictionary.CustomDictionary\")\n",
    "    CustomDictionary.insert(\"阿尔法狗\", \"科技名词 1024\")\n",
    "    \n",
    "    StandardTokenizer = JClass(\"com.hankcs.hanlp.tokenizer.StandardTokenizer\")\n",
    "    StandardTokenizer.SEGMENT.enablePartOfSpeechTagging(True)  # 依然支持隐马词性标注\n",
    "    \n",
    "    term_list = HanLP.segment(\"苹果电脑可以运行开源阿尔法狗代码吗\")\n",
    "    print(term_list)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import doctest\n",
    "    doctest.testmod(verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 依存句法分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_dependency_parser():\n",
    "    \"\"\" 依存句法分析（CRF句法模型需要-Xms512m -Xmx512m -Xmn256m，\n",
    "        MaxEnt和神经网络句法模型需要-Xms1g -Xmx1g -Xmn512m）\n",
    "    >>> demo_dependency_parser()\n",
    "    徐先生 --(主谓关系)--> 帮助\n",
    "    还 --(状中结构)--> 帮助\n",
    "    具体 --(状中结构)--> 帮助\n",
    "    帮助 --(核心关系)--> ##核心##\n",
    "    他 --(兼语)--> 帮助\n",
    "    确定 --(动宾关系)--> 帮助\n",
    "    了 --(右附加关系)--> 确定\n",
    "    把 --(状中结构)--> 作为\n",
    "    画 --(介宾关系)--> 把\n",
    "    雄鹰 --(动宾关系)--> 画\n",
    "    、 --(标点符号)--> 松鼠\n",
    "    松鼠 --(并列关系)--> 雄鹰\n",
    "    和 --(左附加关系)--> 麻雀\n",
    "    麻雀 --(并列关系)--> 雄鹰\n",
    "    作为 --(动宾关系)--> 确定\n",
    "    主攻 --(定中关系)--> 目标\n",
    "    目标 --(动宾关系)--> 作为\n",
    "    。 --(标点符号)--> 帮助\n",
    "    <BLANKLINE>\n",
    "    徐先生 --(主谓关系)--> 帮助\n",
    "    还 --(状中结构)--> 帮助\n",
    "    具体 --(状中结构)--> 帮助\n",
    "    帮助 --(核心关系)--> ##核心##\n",
    "    他 --(兼语)--> 帮助\n",
    "    确定 --(动宾关系)--> 帮助\n",
    "    了 --(右附加关系)--> 确定\n",
    "    把 --(状中结构)--> 作为\n",
    "    画 --(介宾关系)--> 把\n",
    "    雄鹰 --(动宾关系)--> 画\n",
    "    、 --(标点符号)--> 松鼠\n",
    "    松鼠 --(并列关系)--> 雄鹰\n",
    "    和 --(左附加关系)--> 麻雀\n",
    "    麻雀 --(并列关系)--> 雄鹰\n",
    "    作为 --(动宾关系)--> 确定\n",
    "    主攻 --(定中关系)--> 目标\n",
    "    目标 --(动宾关系)--> 作为\n",
    "    。 --(标点符号)--> 帮助\n",
    "    <BLANKLINE>\n",
    "    麻雀 --(并列关系)-->\n",
    "    雄鹰 --(动宾关系)-->\n",
    "    画 --(介宾关系)-->\n",
    "    把 --(状中结构)-->\n",
    "    作为 --(动宾关系)-->\n",
    "    确定 --(动宾关系)-->\n",
    "    帮助 --(核心关系)-->\n",
    "    ##核心##\n",
    "    \"\"\"\n",
    "    sentence = HanLP.parseDependency(\"徐先生还具体帮助他确定了把画雄鹰、松鼠和麻雀作为主攻目标。\")\n",
    "    for word in sentence.iterator():  # 通过dir()可以查看sentence的方法\n",
    "        print(\"%s --(%s)--> %s\" % (word.LEMMA, word.DEPREL, word.HEAD.LEMMA))\n",
    "    print()\n",
    "\n",
    "    # 也可以直接拿到数组，任意顺序或逆序遍历\n",
    "    word_array = sentence.getWordArray()\n",
    "    for word in word_array:\n",
    "        print(\"%s --(%s)--> %s\" % (word.LEMMA, word.DEPREL, word.HEAD.LEMMA))\n",
    "    print()\n",
    "\n",
    "    # 还可以直接遍历子树，从某棵子树的某个节点一路遍历到虚根\n",
    "    CoNLLWord = JClass(\"com.hankcs.hanlp.corpus.dependency.CoNll.CoNLLWord\")\n",
    "    head = word_array[12]\n",
    "    while head.HEAD:\n",
    "        head = head.HEAD\n",
    "        if (head == CoNLLWord.ROOT):\n",
    "            print(head.LEMMA)\n",
    "        else:\n",
    "            print(\"%s --(%s)--> \" % (head.LEMMA, head.DEPREL))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import doctest\n",
    "    doctest.testmod(verbose=True, optionflags=doctest.NORMALIZE_WHITESPACE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 极速分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def demo_high_speed_segment():\n",
    "    \"\"\" 演示极速分词，基于DoubleArrayTrie实现的词典正向最长分词，适用于“高吞吐量”“精度一般”的场合\n",
    "    >>> demo_high_speed_segment()\n",
    "    [江西, 鄱阳湖, 干枯, ，, 中国, 最大, 淡水湖, 变成, 大草原]\n",
    "    SpeedTokenizer分词速度：1253607.32字每秒\n",
    "    \"\"\"\n",
    "    SpeedTokenizer = JClass(\"com.hankcs.hanlp.tokenizer.SpeedTokenizer\")\n",
    "    text = \"江西鄱阳湖干枯，中国最大淡水湖变成大草原\"\n",
    "    JClass(\"com.hankcs.hanlp.HanLP$Config\").ShowTermNature = False\n",
    "    print(SpeedTokenizer.segment(text))\n",
    "\n",
    "    start = time.time()\n",
    "    pressure = 1000000\n",
    "    for i in range(pressure):\n",
    "        SpeedTokenizer.segment(text)\n",
    "    cost_time = time.time() - start\n",
    "    print(\"SpeedTokenizer分词速度：%.2f字每秒\" % (len(text) * pressure / cost_time))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import doctest\n",
    "    doctest.testmod(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 索引分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_index_segment():\n",
    "    \"\"\" 索引分词\n",
    "    >>> demo_index_segment()\n",
    "    主副食品/n [0:4]\n",
    "    主副食/j [0:3]\n",
    "    副食品/n [1:4]\n",
    "    副食/n [1:3]\n",
    "    食品/n [2:4]\n",
    "    最细颗粒度切分：\n",
    "    主副食品/n [0:4]\n",
    "    主副食/j [0:3]\n",
    "    主/ag [0:1]\n",
    "    副食品/n [1:4]\n",
    "    副食/n [1:3]\n",
    "    副/b [1:2]\n",
    "    食品/n [2:4]\n",
    "    食/v [2:3]\n",
    "    品/ng [3:4]\n",
    "    \"\"\"\n",
    "    Term =JClass(\"com.hankcs.hanlp.seg.common.Term\")\n",
    "    IndexTokenizer = JClass(\"com.hankcs.hanlp.tokenizer.IndexTokenizer\")\n",
    "\n",
    "    term_list = IndexTokenizer.segment(\"主副食品\")\n",
    "    for term in term_list.iterator():\n",
    "        print(\"{} [{}:{}]\".format(term, term.offset, term.offset + len(term.word)))\n",
    "\n",
    "    print(\"最细颗粒度切分：\")\n",
    "    IndexTokenizer.SEGMENT.enableIndexMode(JInt(1))  # JInt用于区分重载\n",
    "    term_list = IndexTokenizer.segment(\"主副食品\")\n",
    "    for term in term_list.iterator():\n",
    "        print(\"{} [{}:{}]\".format(term, term.offset, term.offset + len(term.word)))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import doctest\n",
    "    doctest.testmod(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 日本人名识别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_japanese_name_recognition(sentences):\n",
    "    \"\"\" 日本人名识别\n",
    "    >>> sentences =[\n",
    "    ...    \"北川景子参演了林诣彬导演的《速度与激情3》\",\n",
    "    ...    \"林志玲亮相网友:确定不是波多野结衣？\",\n",
    "    ...    \"龟山千广和近藤公园在龟山公园里喝酒赏花\",\n",
    "    ... ]\n",
    "    >>> demo_japanese_name_recognition(sentences)\n",
    "    [北川景子/nrj, 参演/v, 了/ule, 林诣彬/nr, 导演/nnt, 的/ude1, 《/w, 速度/n, 与/cc, 激情/n, 3/m, 》/w]\n",
    "    [林志玲/nr, 亮相/vi, 网友/n, :/w, 确定/v, 不是/c, 波多野结衣/nrj, ？/w]\n",
    "    [龟山千广/nrj, 和/cc, 近藤公园/nrj, 在/p, 龟山/nz, 公园/n, 里/f, 喝酒/vi, 赏花/nz]\n",
    "    \"\"\"\n",
    "    Segment = JClass(\"com.hankcs.hanlp.seg.Segment\")\n",
    "    Term = JClass(\"com.hankcs.hanlp.seg.common.Term\")\n",
    "\n",
    "    segment = HanLP.newSegment().enableJapaneseNameRecognize(True)\n",
    "    for sentence in sentences:\n",
    "        term_list = segment.seg(sentence)\n",
    "        print(term_list)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import doctest\n",
    "    doctest.testmod(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 关键词提取TextRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_keyword(content):\n",
    "    \"\"\" 关键词提取\n",
    "    >>> content = (\n",
    "    ...    \"程序员(英文Programmer)是从事程序开发、维护的专业人员。\"\n",
    "    ...    \"一般将程序员分为程序设计人员和程序编码人员，\"\n",
    "    ...    \"但两者的界限并不非常清楚，特别是在中国。\"\n",
    "    ...    \"软件从业人员分为初级程序员、高级程序员、系统\"\n",
    "    ...    \"分析员和项目经理四大类。\")\n",
    "    >>> demo_keyword(content)\n",
    "    [程序员, 程序, 分为, 人员, 软件]\n",
    "    \"\"\"\n",
    "    TextRankKeyword = JClass(\"com.hankcs.hanlp.summary.TextRankKeyword\")\n",
    "    keyword_list = HanLP.extractKeyword(content, 5)\n",
    "    print(keyword_list)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import doctest\n",
    "    doctest.testmod(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 演示正规化字符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_normalization():\n",
    "    \"\"\" 演示正规化字符配置项的效果（繁体->简体，全角->半角，大写->小写）。\n",
    "        该配置项位于hanlp.properties中，通过Normalization=true来开启\n",
    "        切换配置后必须删除CustomDictionary.txt.bin缓存，否则只影响动态插入的新词。\n",
    "    >>> demo_normalization()\n",
    "    [爱听4g/nz]\n",
    "    [爱听4g/nz]\n",
    "    [爱听4g/nz]\n",
    "    [爱听4g/nz]\n",
    "    [爱听4g/nz]\n",
    "    \"\"\"\n",
    "    CustomDictionary =JClass(\"com.hankcs.hanlp.dictionary.CustomDictionary\")\n",
    "    Config = JClass(\"com.hankcs.hanlp.HanLP$Config\")\n",
    "\n",
    "    Config.Normalization = True\n",
    "    CustomDictionary.insert(\"爱听4G\", \"nz 1000\")\n",
    "    print(HanLP.segment(\"爱听4g\"))\n",
    "    print(HanLP.segment(\"爱听4G\"))\n",
    "    print(HanLP.segment(\"爱听４G\"))\n",
    "    print(HanLP.segment(\"爱听４Ｇ\"))\n",
    "    print(HanLP.segment(\"愛聽４Ｇ\"))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import doctest\n",
    "    doctest.testmod(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 自动去除停用词、自动断句的分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_notional_tokenizer():\n",
    "    \"\"\" 演示自动去除停用词、自动断句的分词器\n",
    "    >>> demo_notional_tokenizer()\n",
    "    [小区/n, 居民/n, 反对/v, 喂养/v, 流浪猫/nz, 居民/n, 赞成/v, 喂养/v, 小宝贝/nz]\n",
    "    [小区/n, 居民/n, 反对/v, 喂养/v, 流浪猫/nz]\n",
    "    [居民/n, 赞成/v, 喂养/v, 小宝贝/nz]\n",
    "    \"\"\"\n",
    "    Term =JClass(\"com.hankcs.hanlp.seg.common.Term\")\n",
    "    NotionalTokenizer = JClass(\"com.hankcs.hanlp.tokenizer.NotionalTokenizer\")\n",
    "\n",
    "    text = \"小区居民有的反对喂养流浪猫，而有的居民却赞成喂养这些小宝贝\"\n",
    "    print(NotionalTokenizer.segment(text))\n",
    "    \n",
    "    for sentence in NotionalTokenizer.seg2sentence(text):\n",
    "        print(sentence)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import doctest\n",
    "    doctest.testmod(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数词和数量词识别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_number_and_quantifier_recognition(sentences):\n",
    "    \"\"\" 演示数词和数量词识别\n",
    "    >>> sentences = [\n",
    "    ...    \"十九元套餐包括什么\",\n",
    "    ...    \"九千九百九十九朵玫瑰\",\n",
    "    ...    \"壹佰块都不给我\",\n",
    "    ...    \"９０１２３４５６７８只蚂蚁\",\n",
    "    ...    \"牛奶三〇〇克*2\",\n",
    "    ...    \"ChinaJoy“扫黄”细则露胸超2厘米罚款\",\n",
    "    ... ]\n",
    "    >>> demo_number_and_quantifier_recognition(sentences)\n",
    "    [十九元/mq, 套餐/n, 包括/v, 什么/ry]\n",
    "    [九千九百九十九朵/mq, 玫瑰/n]\n",
    "    [壹佰块/mq, 都/d, 不/d, 给/p, 我/rr]\n",
    "    [９０１２３４５６７８只/mq, 蚂蚁/n]\n",
    "    [牛奶/nf, 三〇〇克/mq, */w, 2/m]\n",
    "    [ChinaJoy/nx, “/w, 扫黄/vi, ”/w, 细则/n, 露/v, 胸/ng, 超/v, 2厘米/mq, 罚款/vi]\n",
    "    \"\"\"\n",
    "    StandardTokenizer = JClass(\"com.hankcs.hanlp.tokenizer.StandardTokenizer\")\n",
    "\n",
    "    StandardTokenizer.SEGMENT.enableNumberQuantifierRecognize(True)\n",
    "    for sentence in sentences:\n",
    "        print(StandardTokenizer.segment(sentence))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import doctest\n",
    "    doctest.testmod(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 词共现统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_occurrence():\n",
    "    \"\"\" 演示词共现统计\n",
    "    >>> demo_occurrence()\n",
    "    信息=1\n",
    "    先进=1\n",
    "    图形图像=1\n",
    "    处理=2\n",
    "    技术=1\n",
    "    方面=1\n",
    "    比较=1\n",
    "    目前=1\n",
    "    算法=2\n",
    "    视频=1\n",
    "    计算机=1\n",
    "    音视频=1\n",
    "    <BLANKLINE>\n",
    "    信息→算法= tf=1 mi=8.856243954648566 le=0.0 re=0.0 score=NaN\n",
    "    先进→视频= tf=1 mi=6.594180024229758 le=0.0 re=0.0 score=NaN\n",
    "    图形图像→技术= tf=1 mi=20.46090157247892 le=0.0 re=0.0 score=NaN\n",
    "    处理→方面= tf=1 mi=4.04319404601706 le=0.0 re=0.0 score=NaN\n",
    "    处理→算法= tf=1 mi=9.247593120777918 le=0.0 re=0.0 score=NaN\n",
    "    技术→信息= tf=1 mi=4.012478779454232 le=0.0 re=0.0 score=NaN\n",
    "    方面→目前= tf=1 mi=12.825210015738996 le=0.0 re=0.0 score=NaN\n",
    "    比较→先进= tf=1 mi=6.050081533887511 le=0.0 re=0.0 score=NaN\n",
    "    目前→比较= tf=1 mi=13.377862072309142 le=0.0 re=0.0 score=NaN\n",
    "    算法→处理= tf=1 mi=9.247593120777918 le=0.0 re=0.0 score=NaN\n",
    "    视频→处理= tf=1 mi=5.139944592929454 le=0.0 re=0.0 score=NaN\n",
    "    计算机→音视频= tf=1 mi=20.46090157247892 le=0.0 re=0.0 score=NaN\n",
    "    音视频→图形图像= tf=1 mi=20.46090157247892 le=0.0 re=0.0 score=NaN\n",
    "    <BLANKLINE>\n",
    "    信息→算法→处理= tf=1 mi=0.0 le=0.0 re=0.0\n",
    "    先进→视频→处理= tf=1 mi=0.0 le=0.0 re=0.0\n",
    "    图形图像→技术→信息= tf=1 mi=0.0 le=0.0 re=0.0\n",
    "    处理→方面→目前= tf=1 mi=0.0 le=0.0 re=0.0\n",
    "    技术→信息→算法= tf=1 mi=0.0 le=0.0 re=0.0\n",
    "    方面→目前→比较= tf=1 mi=0.0 le=0.0 re=0.0\n",
    "    比较→先进→视频= tf=1 mi=0.0 le=0.0 re=0.0\n",
    "    目前→比较→先进= tf=1 mi=0.0 le=0.0 re=0.0\n",
    "    算法→处理→方面= tf=1 mi=0.0 le=0.0 re=0.0\n",
    "    视频→处理→算法= tf=1 mi=0.0 le=0.0 re=0.0\n",
    "    计算机→音视频→图形图像= tf=1 mi=0.0 le=0.0 re=0.0\n",
    "    音视频→图形图像→技术= tf=1 mi=0.0 le=0.0 re=0.0\n",
    "    \"\"\"\n",
    "    Occurrence = JClass(\"com.hankcs.hanlp.corpus.occurrence.Occurrence\")\n",
    "    PairFrequency = JClass(\"com.hankcs.hanlp.corpus.occurrence.PairFrequency\")\n",
    "    TermFrequency = JClass(\"com.hankcs.hanlp.corpus.occurrence.TermFrequency\")\n",
    "    TriaFrequency = JClass(\"com.hankcs.hanlp.corpus.occurrence.TriaFrequency\")\n",
    "\n",
    "    occurrence = Occurrence()\n",
    "    occurrence.addAll(\"在计算机音视频和图形图像技术等二维信息算法处理方面目前比较先进的视频处理算法\")\n",
    "    occurrence.compute()\n",
    "\n",
    "    unigram = occurrence.getUniGram()\n",
    "    for entry in unigram.iterator():\n",
    "        term_frequency = entry.getValue()\n",
    "        print(term_frequency)\n",
    "    print()\n",
    "\n",
    "    bigram = occurrence.getBiGram()\n",
    "    for entry in bigram.iterator():\n",
    "        pair_frequency = entry.getValue()\n",
    "        if pair_frequency.isRight():\n",
    "            print(pair_frequency)\n",
    "    print()\n",
    "\n",
    "    trigram = occurrence.getTriGram()\n",
    "    for entry in trigram.iterator():\n",
    "        tria_frequency = entry.getValue()\n",
    "        if tria_frequency.isRight():\n",
    "            print(tria_frequency)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import doctest\n",
    "    doctest.testmod(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 机构名识别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_organization_recognition(sentences):\n",
    "    \"\"\" 机构名识别\n",
    "    >>> sentences = [\n",
    "    ...    \"我在上海林原科技有限公司兼职工作，\",\n",
    "    ...    \"我经常在台川喜宴餐厅吃饭，\",\n",
    "    ...    \"偶尔去开元地中海影城看电影。\",\n",
    "    ... ]\n",
    "    >>> demo_organization_recognition(sentences)\n",
    "    [我/rr, 在/p, 上海/ns, 林原科技有限公司/nt, 兼职/vn, 工作/vn, ，/w]\n",
    "    [我/rr, 经常/d, 在/p, 台川喜宴餐厅/nt, 吃饭/vi, ，/w]\n",
    "    [偶尔/d, 去/vf, 开元地中海影城/nt, 看/v, 电影/n, 。/w]\n",
    "    \"\"\"\n",
    "    Segment = JClass(\"com.hankcs.hanlp.seg.Segment\")\n",
    "    Term = JClass(\"com.hankcs.hanlp.seg.common.Term\")\n",
    "\n",
    "    segment = HanLP.newSegment().enableOrganizationRecognize(True)\n",
    "    for sentence in sentences:\n",
    "        term_list = segment.seg(sentence)\n",
    "        print(term_list)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import doctest\n",
    "    doctest.testmod(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 基于感知机序列标注的词法分析器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_perceptron_lexical_analyzer():\n",
    "    \"\"\" 基于感知机序列标注的词法分析器，默认模型训练自1998人民日报语料1月份。欢迎在更大的语料库上训练，以得到更好的效果。\n",
    "        无论在何种语料上训练，都完全支持简繁全半角和大小写。\n",
    "    >>> demo_perceptron_lexical_analyzer()\n",
    "    [上海/ns 华安/nz 工业/n （/w 集团/n ）/w 公司/n]/nt 董事长/n 谭旭光/nr 和/c 秘书/n\n",
    "        胡花蕊/nr 来到/v [美国/ns 纽约/ns 现代/t 艺术/n 博物馆/n]/ns 参观/v\n",
    "    [微软/nt 公司/n]/nt 於/p 1975年/t 由/p 比爾·蓋茲/n 和/c 保羅·艾倫/nr 創立/v ，/w\n",
    "        18年/t 啟動/v 以/p 智慧/n 雲端/n 、/w 前端/f 為/v 導向/n 的/u 大/a 改組/vn 。/w\n",
    "    总统/n 普京/nr 与/c 特朗/b 普通/a 电话/n 讨论/v 太空/s 探索/vn 技术/n 公司/n\n",
    "    总统/n 普京/nr 与/c 特朗普/nr 通/v 电话/n 讨论/v [太空/s 探索/vn 技术/n 公司/n]/nt\n",
    "    主席/n 和/c 特朗普/nr 通/v 电话/n\n",
    "    我/r 在/p 浙江/ns 金华/nr 出生/v\n",
    "    我/r 在/p 四川/ns 金华/ns 出生/v ，/w 我/r 的/u 名字/n 叫/v 金华/nr\n",
    "    空格 /d\n",
    "    /v\n",
    "    /n 统统/d 都/d 不要/d\n",
    "    \"\"\"\n",
    "    PerceptronLexicalAnalyzer = JClass(\"com.hankcs.hanlp.model.perceptron.PerceptronLexicalAnalyzer\")\n",
    "    analyzer = PerceptronLexicalAnalyzer()\n",
    "\n",
    "    print(analyzer.analyze(\"上海华安工业（集团）公司董事长谭旭光和秘书胡花蕊来到美国纽约现代艺术博物馆参观\"))\n",
    "    print(analyzer.analyze(\"微软公司於1975年由比爾·蓋茲和保羅·艾倫創立，18年啟動以智慧雲端、前端為導向的大改組。\"))\n",
    "\n",
    "    # 任何模型总会有失误，特别是98年这种陈旧的语料库\n",
    "    print(analyzer.analyze(\"总统普京与特朗普通电话讨论太空探索技术公司\"))\n",
    "    # 支持在线学习\n",
    "    analyzer.learn(\"与/c 特朗普/nr 通/v 电话/n 讨论/v [太空/s 探索/vn 技术/n 公司/n]/nt\")\n",
    "    # 学习到新知识\n",
    "    print(analyzer.analyze(\"总统普京与特朗普通电话讨论太空探索技术公司\"))\n",
    "    # 还可以举一反三\n",
    "    print(analyzer.analyze(\"主席和特朗普通电话\"))\n",
    "\n",
    "    # 知识的泛化不是死板的规则，而是比较灵活的统计信息\n",
    "    print(analyzer.analyze(\"我在浙江金华出生\"))\n",
    "    analyzer.learn(\"在/p 浙江/ns 金华/ns 出生/v\")\n",
    "    print(analyzer.analyze(\"我在四川金华出生，我的名字叫金华\"))\n",
    "\n",
    "    # 请用户按需执行对空格制表符等的预处理，只有你最清楚自己的文本中都有些什么奇怪的东西\n",
    "    print(analyzer.analyze(\"空格 \\t\\n\\r\\f&nbsp;统统都不要\"\n",
    "        .replace(\"\\\\s+\", \"\")    # 去除所有空白符\n",
    "        .replace(\"&nbsp;\", \"\")  # 如果一些文本中含有html控制符\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 汉字转拼音"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_pinyin():\n",
    "    \"\"\" 汉字转拼音\n",
    "    >>> demo_pinyin()\n",
    "    原文， 重载不是重任！\n",
    "    拼音（数字音调）， [chong2, zai3, bu2, shi4, zhong4, ren4, none5]\n",
    "    拼音（符号音调）， chóng, zǎi, bú, shì, zhòng, rèn, none,\n",
    "    拼音（无音调）， chong, zai, bu, shi, zhong, ren, none,\n",
    "    声调， 2, 3, 2, 4, 4, 4, 5,\n",
    "    声母， ch, z, b, sh, zh, r, none,\n",
    "    韵母， ong, ai, u, i, ong, en, none,\n",
    "    输入法头， ch, z, b, sh, zh, r, none,\n",
    "    jie zhi none none none none nian none\n",
    "    jie zhi 2 0 1 2 nian ，\n",
    "    \"\"\"\n",
    "    Pinyin = JClass(\"com.hankcs.hanlp.dictionary.py.Pinyin\")\n",
    "    text = \"重载不是重任！\"\n",
    "    pinyin_list = HanLP.convertToPinyinList(text)\n",
    "\n",
    "    print(\"原文，\", end=\" \")\n",
    "    print(text)\n",
    "    print(\"拼音（数字音调），\", end=\" \")\n",
    "    print(pinyin_list)\n",
    "    print(\"拼音（符号音调），\", end=\" \")\n",
    "    for pinyin in pinyin_list:\n",
    "        print(\"%s,\" % pinyin.getPinyinWithToneMark(), end=\" \")\n",
    "    print(\"\\n拼音（无音调），\", end=\" \")\n",
    "    for pinyin in pinyin_list:\n",
    "        print(\"%s,\" % pinyin.getPinyinWithoutTone(), end=\" \")\n",
    "    print(\"\\n声调，\", end=\" \")\n",
    "    for pinyin in pinyin_list:\n",
    "        print(\"%s,\" % pinyin.getTone(), end=\" \")\n",
    "    print(\"\\n声母，\", end=\" \")\n",
    "    for pinyin in pinyin_list:\n",
    "        print(\"%s,\" % pinyin.getShengmu(), end=\" \")\n",
    "    print(\"\\n韵母，\", end=\" \")\n",
    "    for pinyin in pinyin_list:\n",
    "        print(\"%s,\" % pinyin.getYunmu(), end=\" \")\n",
    "    print(\"\\n输入法头，\", end=\" \")\n",
    "    for pinyin in pinyin_list:\n",
    "        print(\"%s,\" % pinyin.getHead(), end=\" \")\n",
    "\n",
    "    print()\n",
    "    # 拼音转换可选保留无拼音的原字符\n",
    "    print(HanLP.convertToPinyinString(\"截至2012年，\", \" \", True))\n",
    "    print(HanLP.convertToPinyinString(\"截至2012年，\", \" \", False))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import doctest\n",
    "    doctest.testmod(verbose=True, optionflags=doctest.NORMALIZE_WHITESPACE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pinyin_to_chinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_pinyin_to_chinese():\n",
    "    \"\"\" HanLP中的数据结构和接口是灵活的，组合这些接口，可以自己创造新功能\n",
    "    >>> demo_pinyin_to_chinese()\n",
    "    [renmenrenweiyalujiangbujian/null, lvse/[滤色, 绿色]]\n",
    "    \"\"\"\n",
    "    AhoCorasickDoubleArrayTrie = JClass(\n",
    "        \"com.hankcs.hanlp.collection.AhoCorasick.AhoCorasickDoubleArrayTrie\")\n",
    "    StringDictionary = JClass(\n",
    "        \"com.hankcs.hanlp.corpus.dictionary.StringDictionary\")\n",
    "    CommonAhoCorasickDoubleArrayTrieSegment = JClass(\n",
    "        \"com.hankcs.hanlp.seg.Other.CommonAhoCorasickDoubleArrayTrieSegment\")\n",
    "    CommonAhoCorasickSegmentUtil = JClass(\n",
    "        \"com.hankcs.hanlp.seg.Other.CommonAhoCorasickSegmentUtil\")\n",
    "    Config = JClass(\"com.hankcs.hanlp.HanLP$Config\")\n",
    "\n",
    "    TreeMap = JClass(\"java.util.TreeMap\")\n",
    "    TreeSet = JClass(\"java.util.TreeSet\")\n",
    "\n",
    "    dictionary = StringDictionary()\n",
    "    dictionary.load(Config.PinyinDictionaryPath)\n",
    "    entry = {}\n",
    "    m_map = TreeMap()\n",
    "    for entry in dictionary.entrySet():\n",
    "        pinyins = entry.getValue().replace(\"[\\\\d,]\", \"\")\n",
    "        words = m_map.get(pinyins)\n",
    "        if words is None:\n",
    "            words = TreeSet()\n",
    "            m_map.put(pinyins, words)\n",
    "        words.add(entry.getKey())\n",
    "    words = TreeSet()\n",
    "    words.add(\"绿色\")\n",
    "    words.add(\"滤色\")\n",
    "    m_map.put(\"lvse\", words)\n",
    "\n",
    "    segment = CommonAhoCorasickDoubleArrayTrieSegment(m_map)\n",
    "    print(segment.segment(\"renmenrenweiyalujiangbujianlvse\"))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import doctest\n",
    "    doctest.testmod(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 地名识别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_place_recognition(sentences):\n",
    "    \"\"\" 地名识别\n",
    "    >>> sentences = [\"蓝翔给宁夏固原市彭阳县红河镇黑牛沟村捐赠了挖掘机\"]\n",
    "    >>> demo_place_recognition(sentences)\n",
    "    [蓝翔/nt, 给/p, 宁夏/ns, 固原市/ns, 彭阳县/ns, 红河镇/ns, 黑牛沟村/ns, 捐赠/v, 了/ule, 挖掘机/n]\n",
    "    \"\"\"\n",
    "    Segment = JClass(\"com.hankcs.hanlp.seg.Segment\")\n",
    "    Term = JClass(\"com.hankcs.hanlp.seg.common.Term\")\n",
    "\n",
    "    segment = HanLP.newSegment().enablePlaceRecognize(True)\n",
    "    for sentence in sentences:\n",
    "        term_list = segment.seg(sentence)\n",
    "        print(term_list)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import doctest\n",
    "    doctest.testmod(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 词性标注"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_pos_tagging():\n",
    "    \"\"\" 词性标注\n",
    "    >>> demo_pos_tagging()\n",
    "    未标注： [教授/nnt, 正在/d, 教授/nnt, 自然语言处理/nz, 课程/n]\n",
    "    标注后： [教授/nnt, 正在/d, 教授/v, 自然语言处理/nz, 课程/n]\n",
    "    \"\"\"\n",
    "    Segment = JClass(\"com.hankcs.hanlp.seg.Segment\")\n",
    "    text = \"教授正在教授自然语言处理课程\"\n",
    "    segment = HanLP.newSegment()\n",
    "\n",
    "    print(\"未标注：\", segment.seg(text))\n",
    "    segment.enablePartOfSpeechTagging(True)\n",
    "    print(\"标注后：\", segment.seg(text))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import doctest\n",
    "    doctest.testmod(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 同义改写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_rewrite_text():\n",
    "    \"\"\" 同义改写\n",
    "    >>> demo_rewrite_text()\n",
    "    此点子可以使同义词词典将一律段文本改写成意思相似之另一样段落文本，而且多符合语法\n",
    "    \"\"\"\n",
    "    CoreSynonymDictionary = JClass(\"com.hankcs.hanlp.dictionary.CoreSynonymDictionary\")\n",
    "    text = \"\"\n",
    "    print(CoreSynonymDictionary.rewrite(text))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import doctest\n",
    "    doctest.testmod(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 情感分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tests.test_utility import ensure_data\n",
    "\n",
    "IClassifier = JClass('com.hankcs.hanlp.classification.classifiers.IClassifier')\n",
    "NaiveBayesClassifier = JClass('com.hankcs.hanlp.classification.classifiers.NaiveBayesClassifier')\n",
    "# 中文情感挖掘语料-ChnSentiCorp 谭松波\n",
    "chn_senti_corp = ensure_data(\"ChnSentiCorp情感分析酒店评论\", \"http://hanlp.linrunsoft.com/release/corpus/ChnSentiCorp.zip\")\n",
    "\n",
    "\n",
    "def predict(classifier, text):\n",
    "    print(\"《%s》 情感极性是 【%s】\" % (text, classifier.classify(text)))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    classifier = NaiveBayesClassifier()\n",
    "    #  创建分类器，更高级的功能请参考IClassifier的接口定义\n",
    "    classifier.train(chn_senti_corp)\n",
    "    #  训练后的模型支持持久化，下次就不必训练了\n",
    "    predict(classifier, \"前台客房服务态度非常好！早餐很丰富，房价很干净。再接再厉！\")\n",
    "    predict(classifier, \"结果大失所望，灯光昏暗，空间极其狭小，床垫质量恶劣，房间还伴着一股霉味。\")\n",
    "    predict(classifier, \"可利用文本分类实现情感分析，效果不是不行\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 添加stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在import pyhanlp之前编译自己的Java class，并放入pyhanlp/static中\n",
    "import os\n",
    "\n",
    "from pyhanlp.static import STATIC_ROOT, HANLP_JAR_PATH\n",
    "\n",
    "java_code_path = os.path.join(STATIC_ROOT, 'MyFilter.java')\n",
    "with open(java_code_path, 'w') as out:\n",
    "    java_code = \"\"\"\n",
    "        import com.hankcs.hanlp.dictionary.stopword.CoreStopWordDictionary;\n",
    "        import com.hankcs.hanlp.dictionary.stopword.Filter;\n",
    "        import com.hankcs.hanlp.seg.common.Term;\n",
    "        public class MyFilter implements Filter\n",
    "        {\n",
    "            public boolean shouldInclude(Term term)\n",
    "            {\n",
    "                if (term.nature.startsWith('m')) return true; // 数词保留\n",
    "                return !CoreStopWordDictionary.contains(term.word); // 停用词过滤\n",
    "            }\n",
    "        }\n",
    "        \"\"\"\n",
    "    out.write(java_code)\n",
    "os.system('javac -cp {} {} -d {}'.format(HANLP_JAR_PATH, java_code_path, STATIC_ROOT))\n",
    "# 编译结束才可以启动hanlp\n",
    "from pyhanlp import *\n",
    "\n",
    "\n",
    "def demo_stopword():\n",
    "    \"\"\"\n",
    "    >>> demo_stopword()\n",
    "    [小区/n, 反对/v, 喂养/v, 流浪猫/nz, 赞成/v, 喂养/v, 小宝贝/nz]\n",
    "    [小区/n, 居民/n, 反对/v, 喂养/v, 流浪猫/nz, 居民/n, 赞成/v, 喂养/v, 小宝贝/nz]\n",
    "    [小区/n, 居民/n, 有/vyou, 的/ude1, 反对/v, 喂养/v, 流浪猫/nz, ，/w, 而/cc, 有的/rz, 居民/n, 却/d, 赞成/v, 喂养/v, 这些/rz, 小宝贝/nz]\n",
    "    [小区/n, 居民/n, 反对/v, 喂养/v, 流浪猫/nz, 居民/n, 赞成/v, 喂养/v, 小宝贝/nz]\n",
    "    [数字/n, 123/m, 保留/v]\n",
    "    \"\"\"\n",
    "    CoreStopWordDictionary = JClass(\"com.hankcs.hanlp.dictionary.stopword.CoreStopWordDictionary\")\n",
    "    Filter = JClass(\"com.hankcs.hanlp.dictionary.stopword.Filter\")\n",
    "    Term = JClass(\"com.hankcs.hanlp.seg.common.Term\")\n",
    "    BasicTokenizer = JClass(\"com.hankcs.hanlp.tokenizer.BasicTokenizer\")\n",
    "    NotionalTokenizer = JClass(\"com.hankcs.hanlp.tokenizer.NotionalTokenizer\")\n",
    "\n",
    "    text = \"小区居民有的反对喂养流浪猫，而有的居民却赞成喂养这些小宝贝\"\n",
    "    # 可以动态修改停用词词典\n",
    "    CoreStopWordDictionary.add(\"居民\")\n",
    "    print(NotionalTokenizer.segment(text))\n",
    "    CoreStopWordDictionary.remove(\"居民\")\n",
    "    print(NotionalTokenizer.segment(text))\n",
    "\n",
    "    # 可以对任意分词器的结果执行过滤\n",
    "    term_list = BasicTokenizer.segment(text)\n",
    "    print(term_list)\n",
    "    CoreStopWordDictionary.apply(term_list)\n",
    "    print(term_list)\n",
    "\n",
    "    # 还可以自定义过滤逻辑\n",
    "    MyFilter = JClass('MyFilter')\n",
    "    CoreStopWordDictionary.FILTER = MyFilter()\n",
    "    print(NotionalTokenizer.segment(\"数字123的保留\"))  # “的”位于stopwords.txt所以被过滤，数字得到保留\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import doctest\n",
    "\n",
    "    doctest.testmod(verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 文本推荐(句子级别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_suggester():\n",
    "    \"\"\" 文本推荐(句子级别，从一系列句子中挑出与输入句子最相似的那一个)\n",
    "    >>> demo_suggester()\n",
    "    [威廉王子发表演说 呼吁保护野生动物, 英报告说空气污染带来“公共健康危机”]\n",
    "    [英报告说空气污染带来“公共健康危机”]\n",
    "    [《时代》年度人物最终入围名单出炉 普京马云入选]\n",
    "    [魅惑天后许佳慧不爱“预谋” 独唱《许某某》]\n",
    "    \"\"\"\n",
    "    Suggester = JClass(\"com.hankcs.hanlp.suggest.Suggester\")\n",
    "    suggester = Suggester()\n",
    "    title_array = [\n",
    "        \"威廉王子发表演说 呼吁保护野生动物\",\n",
    "        \"魅惑天后许佳慧不爱“预谋” 独唱《许某某》\",\n",
    "        \"《时代》年度人物最终入围名单出炉 普京马云入选\",\n",
    "        \"“黑格比”横扫菲：菲吸取“海燕”经验及早疏散\",\n",
    "        \"日本保密法将正式生效 日媒指其损害国民知情权\",\n",
    "        \"英报告说空气污染带来“公共健康危机”\"\n",
    "    ]\n",
    "    for title in title_array:\n",
    "        suggester.addSentence(title)\n",
    "\n",
    "    print(suggester.suggest(\"陈述\", 2))      # 语义\n",
    "    print(suggester.suggest(\"危机公关\", 1))  # 字符\n",
    "    print(suggester.suggest(\"mayun\", 1))   # 拼音\n",
    "    print(suggester.suggest(\"徐家汇\", 1)) # 拼音\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import doctest\n",
    "    doctest.testmod(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 文本分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhanlp import SafeJClass\n",
    "from tests.test_utility import ensure_data\n",
    "\n",
    "NaiveBayesClassifier = SafeJClass('com.hankcs.hanlp.classification.classifiers.NaiveBayesClassifier')\n",
    "IOUtil = SafeJClass('com.hankcs.hanlp.corpus.io.IOUtil')\n",
    "sogou_corpus_path = ensure_data('搜狗文本分类语料库迷你版',\n",
    "                                'http://hanlp.linrunsoft.com/release/corpus/sogou-text-classification-corpus-mini.zip')\n",
    "\n",
    "\n",
    "def train_or_load_classifier():\n",
    "    model_path = sogou_corpus_path + '.ser'\n",
    "    if os.path.isfile(model_path):\n",
    "        return NaiveBayesClassifier(IOUtil.readObjectFrom(model_path))\n",
    "    classifier = NaiveBayesClassifier()\n",
    "    classifier.train(sogou_corpus_path)\n",
    "    model = classifier.getModel()\n",
    "    IOUtil.saveObjectTo(model, model_path)\n",
    "    return NaiveBayesClassifier(model)\n",
    "\n",
    "\n",
    "def predict(classifier, text):\n",
    "    print(\"《%16s》\\t属于分类\\t【%s】\" % (text, classifier.classify(text)))\n",
    "    # 如需获取离散型随机变量的分布，请使用predict接口\n",
    "    # print(\"《%16s》\\t属于分类\\t【%s】\" % (text, classifier.predict(text)))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    classifier = train_or_load_classifier()\n",
    "    predict(classifier, \"C罗获2018环球足球奖最佳球员 德尚荣膺最佳教练\")\n",
    "    predict(classifier, \"英国造航母耗时8年仍未服役 被中国速度远远甩在身后\")\n",
    "    predict(classifier, \"研究生考录模式亟待进一步专业化\")\n",
    "    predict(classifier, \"如果真想用食物解压,建议可以食用燕麦\")\n",
    "    predict(classifier, \"通用及其部分竞争对手目前正在考虑解决库存问题\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 文本聚类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[王五, 赵一], [李四, 钱二], [马六], [张三]]\n",
      "[[张三, 马六], [李四, 王五, 钱二, 赵一]]\n",
      "[[李四, 钱二], [王五, 赵一], [张三, 马六]]\n"
     ]
    }
   ],
   "source": [
    "ClusterAnalyzer = JClass('com.hankcs.hanlp.mining.cluster.ClusterAnalyzer')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    analyzer = ClusterAnalyzer()\n",
    "    analyzer.addDocument(\"赵一\", \"流行, 流行, 流行, 流行, 流行, 流行, 流行, 流行, 流行, 流行, 蓝调, 蓝调, 蓝调, 蓝调, 蓝调, 蓝调, 摇滚, 摇滚, 摇滚, 摇滚\")\n",
    "    analyzer.addDocument(\"钱二\", \"爵士, 爵士, 爵士, 爵士, 爵士, 爵士, 爵士, 爵士, 舞曲, 舞曲, 舞曲, 舞曲, 舞曲, 舞曲, 舞曲, 舞曲, 舞曲\")\n",
    "    analyzer.addDocument(\"张三\", \"古典, 古典, 古典, 古典, 民谣, 民谣, 民谣, 民谣\")\n",
    "    analyzer.addDocument(\"李四\", \"爵士, 爵士, 爵士, 爵士, 爵士, 爵士, 爵士, 爵士, 爵士, 金属, 金属, 舞曲, 舞曲, 舞曲, 舞曲, 舞曲, 舞曲\")\n",
    "    analyzer.addDocument(\"王五\", \"流行, 流行, 流行, 流行, 摇滚, 摇滚, 摇滚, 嘻哈, 嘻哈, 嘻哈\")\n",
    "    analyzer.addDocument(\"马六\", \"古典, 古典, 古典, 古典, 古典, 古典, 古典, 古典, 摇滚\")\n",
    "    print(analyzer.kmeans(4))\n",
    "    print(analyzer.repeatedBisection(2))\n",
    "    print(analyzer.repeatedBisection(1.0))  # 自动判断聚类数量k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 动态设置预置分词器，全局"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_tokenizer_config(text):\n",
    "    \"\"\" 演示动态设置预置分词器，这里的设置是全局的\n",
    "    >>> text = \"泽田依子是上外日本文化经济学院的外教\"\n",
    "    >>> demo_tokenizer_config(text)\n",
    "    [泽田依/nr, 子/ng, 是/vshi, 上外/nit, 日本/ns, 文化/n, 经济学院/nit, 的/ude1, 外教/n]\n",
    "    [泽田依子/nrj, 是/vshi, 上外日本文化经济学院/nt, 的/ude1, 外教/n]\n",
    "    \"\"\"\n",
    "    StandardTokenizer = JClass(\"com.hankcs.hanlp.tokenizer.StandardTokenizer\")\n",
    "    print(StandardTokenizer.segment(text))\n",
    "    StandardTokenizer.SEGMENT.enableAllNamedEntityRecognize(True)\n",
    "    print(StandardTokenizer.segment(text))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import doctest\n",
    "    doctest.testmod(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 将简繁转换做到极致"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_traditional_chinese2simplified_chinese():\n",
    "    \"\"\" 将简繁转换做到极致\n",
    "    >>> demo_traditional_chinese2simplified_chinese()\n",
    "    「以後等你當上皇后，就能買草莓慶祝了」。發現一根白頭髮\n",
    "    凭借笔记本电脑写程序HanLP\n",
    "    hankcs在臺灣寫程式碼\n",
    "    hankcs在台湾写代码\n",
    "    hankcs在香港寫代碼\n",
    "    hankcs在香港写代码\n",
    "    hankcs在臺灣寫程式碼\n",
    "    hankcs在香港寫代碼\n",
    "    hankcs在臺灣寫程式碼\n",
    "    hankcs在台灣寫代碼\n",
    "    hankcs在臺灣寫代碼\n",
    "    hankcs在臺灣寫代碼\n",
    "    \"\"\"\n",
    "    print(HanLP.convertToTraditionalChinese(\"“以后等你当上皇后，就能买草莓庆祝了”。发现一根白头发\"))\n",
    "    print(HanLP.convertToSimplifiedChinese(\"憑藉筆記簿型電腦寫程式HanLP\"))\n",
    "    # 简体转台湾繁体\n",
    "    print(HanLP.s2tw(\"hankcs在台湾写代码\"))\n",
    "    # 台湾繁体转简体\n",
    "    print(HanLP.tw2s(\"hankcs在臺灣寫程式碼\"))\n",
    "    # 简体转香港繁体\n",
    "    print(HanLP.s2hk(\"hankcs在香港写代码\"))\n",
    "    # 香港繁体转简体\n",
    "    print(HanLP.hk2s(\"hankcs在香港寫代碼\"))\n",
    "    # 香港繁体转台湾繁体\n",
    "    print(HanLP.hk2tw(\"hankcs在臺灣寫代碼\"))\n",
    "    # 台湾繁体转香港繁体\n",
    "    print(HanLP.tw2hk(\"hankcs在香港寫程式碼\"))\n",
    "\n",
    "    # 香港/台湾繁体和HanLP标准繁体的互转\n",
    "    print(HanLP.t2tw(\"hankcs在臺灣寫代碼\"))\n",
    "    print(HanLP.t2hk(\"hankcs在臺灣寫代碼\"))\n",
    "\n",
    "    print(HanLP.tw2t(\"hankcs在臺灣寫程式碼\"))\n",
    "    print(HanLP.hk2t(\"hankcs在台灣寫代碼\"))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import doctest\n",
    "    doctest.testmod(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 音译人名识别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_translated_name_recognition(sentences):\n",
    "    \"\"\" 音译人名识别\n",
    "    >>> sentences = [\n",
    "    ... \"一桶冰水当头倒下，微软的比尔盖茨、Facebook的扎克伯格跟桑德博格、亚马逊的贝索斯、苹果的库克全都不惜湿身入镜，这些硅谷的科技人，飞蛾扑火似地牺牲演出，其实全为了慈善。\",\n",
    "    ... \"世界上最长的姓名是简森·乔伊·亚历山大·比基·卡利斯勒·达夫·埃利奥特·福克斯·伊维鲁莫·马尔尼·梅尔斯·帕特森·汤普森·华莱士·普雷斯顿。\",\n",
    "    ... ]\n",
    "    >>> demo_translated_name_recognition(sentences)\n",
    "    [一桶/nz, 冰水/n, 当头/vi, 倒下/v, ，/w, 微软/ntc, 的/ude1, 比尔盖茨/nrf, 、/w,\n",
    "        Facebook/nx, 的/ude1, 扎克伯格/nrf, 跟/p, 桑德博格/nrf, 、/w, 亚马逊/nrf, 的/ude1,\n",
    "        贝索斯/nrf, 、/w, 苹果/nf, 的/ude1, 库克/nrf, 全都/d, 不惜/v, 湿身/nz, 入镜/nz, ，/w,\n",
    "        这些/rz, 硅谷/ns, 的/ude1, 科技/n, 人/n, ，/w, 飞蛾扑火/nz, 似地/d, 牺牲/v,\n",
    "        演出/vn, ，/w, 其实/d, 全/a, 为了/p, 慈善/a, 。/w]\n",
    "    [世界/n, 上/f, 最长/d, 的/ude1, 姓名/n, 是/vshi,\n",
    "        简森·乔伊·亚历山大·比基·卡利斯勒·达夫·埃利奥特·福克斯·伊维鲁莫·马尔尼·梅尔斯·帕特森·汤普森·华莱士·普雷斯顿/nrf, 。/w]\n",
    "    \"\"\"\n",
    "    segment = HanLP.newSegment().enableTranslatedNameRecognize(True)\n",
    "    for sentence in sentences:\n",
    "        term_list = segment.seg(sentence)\n",
    "        print(term_list)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import doctest\n",
    "    doctest.testmod(verbose=True, optionflags=doctest.NORMALIZE_WHITESPACE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 基于AhoCorasickDoubleArrayTrie的分词器，该分词器允许用户跳过核心词典，直接使用自己的词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_use_AhoCorasickDoubleArrayTrieSegment():\n",
    "    \"\"\" 基于AhoCorasickDoubleArrayTrie的分词器，该分词器允许用户跳过核心词典，直接使用自己的词典。\n",
    "        需要注意的是，自己的词典必须遵守HanLP词典格式。\n",
    "    >>> demo_use_AhoCorasickDoubleArrayTrieSegment()\n",
    "    [微观经济学/nz, 继续教育/nz, 循环经济/nz]\n",
    "    \"\"\"\n",
    "    AhoCorasickDoubleArrayTrieSegment = JClass(\n",
    "        \"com.hankcs.hanlp.seg.Other.AhoCorasickDoubleArrayTrieSegment\")\n",
    "    Config = JClass(\"com.hankcs.hanlp.HanLP$Config\")\n",
    "\n",
    "    # AhoCorasickDoubleArrayTrieSegment要求用户必须提供自己的词典路径\n",
    "    segment = AhoCorasickDoubleArrayTrieSegment(\n",
    "        ).loadDictionary(Config.CustomDictionaryPath[0])\n",
    "    print(segment.seg(\"微观经济学继续教育循环经济\"))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import doctest\n",
    "    doctest.testmod(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 语义距离"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_word_distance():\n",
    "    CoreSynonymDictionary = JClass(\"com.hankcs.hanlp.dictionary.CoreSynonymDictionary\")\n",
    "\n",
    "    word_array = [\n",
    "        \"香蕉\",\n",
    "        \"苹果\",\n",
    "        \"白菜\",\n",
    "        \"水果\",\n",
    "        \"蔬菜\",\n",
    "        \"自行车\",\n",
    "        \"公交车\",\n",
    "        \"飞机\",\n",
    "        \"买\",\n",
    "        \"卖\",\n",
    "        \"购入\",\n",
    "        \"新年\",\n",
    "        \"春节\",\n",
    "        \"丢失\",\n",
    "        \"补办\",\n",
    "        \"办理\",\n",
    "        \"送给\",\n",
    "        \"寻找\",\n",
    "        \"孩子\",\n",
    "        \"教室\",\n",
    "        \"教师\",\n",
    "        \"会计\",\n",
    "    ]\n",
    "    print(\"%-5s\\t%-5s\\t%-10s\\t%-5s\\n\" % (\"词A\", \"词B\", \"语义距离\", \"语义相似度\"))\n",
    "    for a in word_array:\n",
    "        for b in word_array:\n",
    "            print(\"%-5s\\t%-5s\\t%-15d\\t%-5.10f\" % (a, b, CoreSynonymDictionary.distance(a, b),\n",
    "                CoreSynonymDictionary.similarity(a, b)))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import doctest\n",
    "    doctest.testmod(verbose=True, optionflags=doctest.NORMALIZE_WHITESPACE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "363.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
