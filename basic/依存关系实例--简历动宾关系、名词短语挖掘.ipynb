{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 简历中关键动宾关系（动名词短语块）挖掘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "from nltk import , ProbabilisticTree\n",
    "from nltk.chunk.regexp import *\n",
    "import nltk.tree as tree\n",
    "import nltk,re\n",
    "\n",
    "# 安装和教程详见StanfordCoreNLP官网\n",
    "nlp = StanfordCoreNLP(r'D:\\ProgramData\\nlp_package\\stanford-corenlp-full-2018-10-05', \n",
    "                      lang='zh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammer = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "cp = nltk.RegexpParser(grammer)  #生成规则\n",
    "\n",
    "pattern = re.compile(u'[^a-zA-Z\\u4E00-\\u9FA5]')\n",
    "pattern_del = re.compile('(\\a-zA-Z0-9+)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _replace_c(text):\n",
    "    \"\"\"批量替换函数\"\"\"\n",
    "    intab = \",?!()\"       # 替换目标输入\n",
    "    outtab = \"，？！（）\" # 替换后输出\n",
    "    deltab = \" \\n<li>< li>+_-.><li \\U0010fc01 _\"   # 删除目标\n",
    "    trantab = text.maketrans(intab, outtab, deltab)\n",
    "    return text.translate(trantab)\n",
    "\n",
    "\n",
    "def parse_sentence(text):\n",
    "    \"\"\"stanfordnlp解析之后，通过nltk tree返回依存树结构\"\"\"\n",
    "    # 文本过滤替换处理\n",
    "    text = _replace_c(text)\n",
    "    try:\n",
    "        if len(text.strip()) > 6:\n",
    "            return Tree.fromstring(nlp.parse(text.strip()))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "def pos(text):\n",
    "    \"\"\"stanfordnlp词性标注\"\"\"\n",
    "    text = _replace_c(text)\n",
    "    if len(text.strip()) > 6:\n",
    "        return nlp.pos_tag(text)\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def denpency_parse(text):\n",
    "    return nlp.dependency_parse(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    return open(path, \"r\", encoding=\"utf8\")\n",
    "\n",
    "\n",
    "def get_noun_chunk(tree):\n",
    "    \"\"\"合并NP关系节点下的名词子节点\"\"\"\n",
    "    noun_chunk = []\n",
    "    if tree.label() == \"NP\":\n",
    "        nouns_phase = ''.join(tree.leaves())\n",
    "        noun_chunk.append(nouns_phase)\n",
    "    return noun_chunk\n",
    "\n",
    "\n",
    "def get_ip_recursion_noun(tree):\n",
    "    \"\"\"遍历所有IP节点下的名词\"\"\"\n",
    "    np_list = []\n",
    "    if len(tree) == 1:\n",
    "        tr = tree[0]\n",
    "        get_ip_recursion_noun(tr)\n",
    "    if len(tree) == 2:\n",
    "        tr = tree[0]\n",
    "        get_ip_recursion_noun(tr)\n",
    "        tr = tree[1]\n",
    "        get_ip_recursion_noun(tr)\n",
    "    if len(tree) == 3:\n",
    "        tr = tree[0]\n",
    "        get_ip_recursion_noun(tr)\n",
    "        tr = tree[1]\n",
    "        get_ip_recursion_noun(tr)\n",
    "        tr = tree[2]\n",
    "        get_ip_recursion_noun(tr)\n",
    "    if tree.label() == 'NP':\n",
    "        np_list.append(get_noun_chunk(tree))\n",
    "    return np_list\n",
    "\n",
    "\n",
    "def get_vv_loss_np(tree):\n",
    "    \"\"\"遍历传入的非NP节点下的所有名词\"\"\"\n",
    "    if not isinstance(tree, nltk.tree.Tree):\n",
    "        return False\n",
    "    stack = []\n",
    "    np = []\n",
    "    stack.append(tree)\n",
    "    current_tree = ''\n",
    "    while stack:\n",
    "        current_tree = stack.pop()\n",
    "        # 过滤VP\n",
    "        if isinstance(current_tree, nltk.tree.Tree) and current_tree.label() == 'VP':\n",
    "            continue\n",
    "        # 压入非NP节点\n",
    "        elif isinstance(current_tree, nltk.tree.Tree) and current_tree.label() != 'NP':\n",
    "            for i in range(len(current_tree)):\n",
    "                stack.append(current_tree[i])\n",
    "        # chunk NP节点\n",
    "        elif isinstance(current_tree, nltk.tree.Tree) and current_tree.label() == 'NP':\n",
    "            np.append(get_noun_chunk(tree))\n",
    "    if np:\n",
    "        return np\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def search(tree_in):\n",
    "    \"\"\"寻找动宾关系\n",
    "    tree_in：nltk Tree对象\n",
    "    \"\"\"\n",
    "    if not isinstance(tree_in, nltk.tree.Tree):\n",
    "        return False\n",
    "    \n",
    "    vp_pair = []  # 结果列表\n",
    "    stack = []\n",
    "    stack.append(tree_in)\n",
    "    current_tree = ''\n",
    "    \n",
    "    while stack:\n",
    "        tree = stack.pop()\n",
    "        # 根节点，压入栈子节点\n",
    "        if isinstance(tree, nltk.tree.Tree) and tree.label() == \"ROOT\":\n",
    "            for i in range(len(tree)):\n",
    "                stack.append(tree[i])\n",
    "        # 从句节点，压入栈其子节点\n",
    "        if isinstance(tree, nltk.tree.Tree) and tree.label() == \"IP\":\n",
    "            for i in range(len(tree)):\n",
    "                stack.append(tree[i])\n",
    "        # 动宾关系节点，记录vp_pair\n",
    "        if isinstance(tree, nltk.tree.Tree) and tree.label() == \"VP\":\n",
    "            # 防止重复\n",
    "            duplicate = []\n",
    "            if len(tree) >= 2:\n",
    "                for i in range(1, len(tree)):\n",
    "                    if tree[0].label() == 'VV' and tree[i].label() == \"NP\":\n",
    "                        # 合并该节点下动词，不扰乱语义\n",
    "                        verb = ''.join(tree[0].leaves())\n",
    "                        # 合并相邻名词\n",
    "                        noun = get_noun_chunk(tree[i])\n",
    "                        if verb and noun:\n",
    "                            vp_pair.append((verb, noun))\n",
    "                            duplicate.append(noun)\n",
    "                    elif tree[0].label() == 'VV' and tree[i].label() != \"NP\":\n",
    "                        noun = get_vv_loss_np(tree)\n",
    "                        verb = ''.join(tree[0].leaves())\n",
    "                        if verb and noun and noun not in duplicate:\n",
    "                            duplicate.append(noun)\n",
    "                            vp_pair.append((verb, noun))\n",
    "    \n",
    "    if vp_pair:\n",
    "        return vp_pair\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "[('利用', ['各种手段推广公司品牌'])]\n",
      "False\n",
      "False\n",
      "[('落实', ['凝聚力工程'])]\n",
      "[('组织', ['各类公关'])]\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    out = open(\"dependency.txt\",'w',encoding='utf8')\n",
    "    itera = read_data('text.txt')\n",
    "    \n",
    "    for it in itera:\n",
    "        s = parse_sentence(it)\n",
    "        res = search(s)   \n",
    "        print(res)\n",
    "        if not isinstance(res, bool):\n",
    "            out.write(str(res) + '\\n')\n",
    "            \n",
    "    itera.close()\n",
    "    out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 名词短语块挖掘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T11:46:07.125683Z",
     "start_time": "2019-07-02T11:46:05.159156Z"
    }
   },
   "outputs": [],
   "source": [
    "import os,json,nltk,re\n",
    "from pyhanlp import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T11:46:24.744588Z",
     "start_time": "2019-07-02T11:46:24.018985Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所属行业/NP\n",
      "：/O\n",
      "/O\n",
      "快速消费品/NP\n",
      "(/O\n",
      "食品/NP\n",
      ",/O\n",
      "饮料/NP\n",
      ",/O\n",
      "化妆品/NP\n",
      ")/O\n",
      "\n",
      "业务部/NP\n",
      "/O\n",
      "采购主管/NP\n",
      "\n",
      "本人/O\n",
      "是从/VP\n",
      "黄金珠宝营业员/NP\n",
      "开始做起/VP\n",
      ",/O\n",
      "现/O\n",
      "从事/VP\n",
      "黄金珠宝/NP\n",
      "和/O\n",
      "化妆品的招商工作/NP\n",
      ",/O\n",
      "曾/O\n",
      "参与/VP\n",
      "1999/O\n",
      "年/O\n",
      "和/O\n",
      "2004年3家商场的招商工作/NP\n",
      ",/O\n",
      "在/PP\n",
      "公司/NP\n",
      "我/O\n",
      "能/VP\n",
      "比较/O\n",
      "完美/O\n",
      "的/O\n",
      "贯彻/VP\n",
      "和/O\n",
      "执行公司的营销策略/NP\n",
      "和/O\n",
      "销售计划/NP\n",
      ",/O\n",
      "并能提出/VP\n",
      "合理化建议/NP\n",
      ",/O\n",
      "在工作中/O\n",
      "能/VP\n",
      "与/O\n",
      "供应商保持良好/NP\n",
      "的/O\n",
      "合作/VP\n",
      "关系/NP\n",
      "./O\n",
      "\n",
      "RESUMEDOCSSTARTFLAG/NP\n",
      "/O\n",
      "销售总监/NP\n",
      "/O\n",
      "半年市场部总监工作/NP\n",
      "，/O\n",
      "之后/O\n",
      "公司/NP\n",
      "开始/VP\n",
      "华东地区大客户销售业务/NP\n",
      "，/O\n",
      "转为/VP\n",
      "华东区/NP\n",
      "\n",
      "销售总监/NP\n",
      "，/O\n",
      "创建/VP\n",
      "易车网华东地区大客户销售团队/NP\n",
      "，/O\n",
      "并/O\n",
      "带领/VP\n",
      "团队/NP\n",
      "完成/VP\n",
      "公司/NP\n",
      "下达/VP\n",
      "\n",
      "的/O\n",
      "销售任务/NP\n",
      "。/O\n",
      "。/O\n",
      "\n",
      "主要/O\n",
      "收获/NP\n",
      "：/O\n",
      "半年市场总监工作经验/NP\n",
      "，/O\n",
      "成功/O\n",
      "稳固/O\n",
      "公司/NP\n",
      "在/PP\n",
      "上海的品牌形象/NP\n",
      "和/O\n",
      "客户关系/NP\n",
      "，/O\n",
      "有效/O\n",
      "\n",
      "利用/VP\n",
      "各种手段推广公司品牌/NP\n",
      "。/O\n",
      "。/O\n",
      "\n",
      "两年半网络广告销售/NP\n",
      "从业/O\n",
      "及/O\n",
      "管理经验/NP\n",
      "，/O\n",
      "成功/O\n",
      "组建/VP\n",
      "\n",
      "易车网华东区域大客户销售团队/NP\n",
      "，/O\n",
      "以/PP\n",
      "直/O\n",
      "客/NP\n",
      "为/PP\n",
      "导向/NP\n",
      "，/O\n",
      "兼顾/VP\n",
      "渠道/NP\n",
      "。/O\n",
      "。/O\n",
      "\n",
      "有/VP\n",
      "丰富/O\n",
      "的/O\n",
      "互联网/NP\n",
      "\n",
      "广告知识/NP\n",
      "，/O\n",
      "同时/O\n",
      "也/O\n",
      "有/VP\n",
      "华东区域/NP\n",
      "的/O\n",
      "所有/O\n",
      "汽车客户/NP\n",
      "及/O\n",
      "周边产品/NP\n",
      "的/O\n",
      "直/O\n",
      "客关系/NP\n",
      "，/O\n",
      "及/O\n",
      "相/O\n",
      "\n",
      "关的渠道关系/NP\n",
      "。/O\n",
      "。/O\n",
      "\n",
      "RESUMEDOCSSTARTFLAG/NP\n",
      "/O\n",
      "市场部高级经理/NP\n",
      "/O\n",
      "负责东方网/NP\n",
      "旗下/O\n",
      "的/O\n",
      "<东方社区>刊物的市场推广/NP\n",
      "及/O\n",
      "合作/VP\n",
      "\n",
      "主要/O\n",
      "收获/NP\n",
      ":/O\n",
      "/O\n",
      "了解/VP\n",
      "纸媒体特点/NP\n",
      ",/O\n",
      "熟悉/VP\n",
      "期刊编辑/NP\n",
      "出版/VP\n",
      "流程/NP\n",
      ",/O\n",
      "并/O\n",
      "结合/VP\n",
      "活动推广刊物/NP\n",
      "\n",
      "同时/O\n",
      "也/O\n",
      "参与/VP\n",
      "\n",
      "部分平面广告销售工作/NP\n",
      ",/O\n",
      "熟悉/VP\n",
      "平面广告销售/NP\n",
      "的/O\n",
      "所有/O\n",
      "流程/NP\n",
      "\n",
      "RESUMEDOCSSTARTFLAG/NP\n",
      "/O\n",
      "事业发展部主管/NP\n",
      "/O\n",
      "（/O\n",
      "自/PP\n",
      "/O\n",
      "2004/O\n",
      "/O\n",
      "年/O\n",
      "/O\n",
      "3/O\n",
      "/O\n",
      "月/NP\n",
      "——/O\n",
      "2005/O\n",
      "/O\n",
      "年/O\n",
      "/O\n",
      "3/O\n",
      "/O\n",
      "月/NP\n",
      "，/O\n",
      "在/PP\n",
      "共青团上海市委管理信息/NP\n",
      "部/O\n",
      "挂职锻炼/NP\n",
      "）/O\n",
      "网站编辑/NP\n",
      ";/O\n",
      "\n",
      "公司内部活动策划组织/NP\n",
      ",/O\n",
      "项目管理/NP\n",
      ";/O\n",
      "\n",
      "协调/NP\n",
      "./O\n",
      "/O\n",
      "联络/VP\n",
      "并/O\n",
      "保持/VP\n",
      "与/O\n",
      "团市委机关/NP\n",
      "的/O\n",
      "良好/O\n",
      "合作/VP\n",
      "关系/NP\n",
      "\n",
      "主要/O\n",
      "收获/NP\n",
      ":/O\n",
      "/O\n",
      "对/PP\n",
      "网络与信息化/NP\n",
      "有/VP\n",
      "了/O\n",
      "进一步/O\n",
      "的/O\n",
      "认识/VP\n",
      ",/O\n",
      "了解/VP\n",
      "网站的运营模式/NP\n",
      "\n",
      "在/PP\n",
      "团市委机关挂职/NP\n",
      "\n",
      "期间/O\n",
      ",/O\n",
      "加强/VP\n",
      "了/O\n",
      "自身/O\n",
      "思想道德/NP\n",
      "等/O\n",
      "各/O\n",
      "方面的学习/NP\n",
      ",/O\n",
      "积累/VP\n",
      "了/O\n",
      "广泛/O\n",
      "的/O\n",
      "人脉/NP\n",
      ",/O\n",
      "多次/O\n",
      "参与/VP\n",
      "\n",
      "一些市大型活动的策划组织/NP\n",
      "及/O\n",
      "执行/NP\n",
      ",/O\n",
      "列/VP\n",
      ":/O\n",
      "/O\n",
      "上海/NP\n",
      "/O\n",
      "IT/NP\n",
      "/O\n",
      "青年/NP\n",
      "十大新锐评选活动/NP\n",
      "./O\n",
      "/O\n",
      "上/O\n",
      "\n",
      "海市信息化青年人才协会/NP\n",
      "第一次会员大会--/NP\n",
      "暨/O\n",
      "“/O\n",
      "世博会与信息化/NP\n",
      "”/O\n",
      "青年/NP\n",
      "论/VP\n",
      "\n",
      "坛/NP\n",
      "等等/O\n",
      ",/O\n",
      "使/VP\n",
      "自身/O\n",
      "各/O\n",
      "方面综合素质/NP\n",
      "有/VP\n",
      "了/O\n",
      "一个/O\n",
      "更/O\n",
      "全面/O\n",
      "的/O\n",
      "提高/VP\n",
      "\n",
      "RESUMEDOCSSTARTFLAG/NP\n",
      "/O\n",
      "市场公关专员/NP\n",
      "/O\n",
      "策划组织/NP\n",
      "各类/O\n",
      "活动/NP\n",
      ",/O\n",
      "包括/VP\n",
      "户外拓展/NP\n",
      "训练/VP\n",
      ",/O\n",
      "团队建设/NP\n",
      "等/O\n",
      ";/O\n",
      "\n",
      "策划组织公司/NP\n",
      "内部/O\n",
      "各项/O\n",
      "活动/NP\n",
      ",/O\n",
      "落实/VP\n",
      "凝聚力工程/NP\n",
      ";/O\n",
      "\n",
      "组织/NP\n",
      "各类/O\n",
      "公关接待/NP\n",
      "及/O\n",
      "外联工作/NP\n",
      ";/O\n",
      "\n",
      "各类/O\n",
      "活动的主持/NP\n",
      "及/O\n",
      "执行/NP\n",
      "等/O\n",
      "事宜/NP\n",
      "\n",
      "主要/O\n",
      "收获/NP\n",
      ":/O\n",
      "/O\n",
      "积累/VP\n",
      "了/O\n",
      "更/O\n",
      "丰富/O\n",
      "的/O\n",
      "策划经验/NP\n",
      ",/O\n",
      "多次/O\n",
      "的/O\n",
      "主持经历/NP\n",
      "让/VP\n",
      "我/O\n",
      "在/PP\n",
      "语言表达方面/NP\n",
      "有/VP\n",
      "了/O\n",
      "进一步/O\n",
      "\n",
      "的/O\n",
      "提高/VP\n",
      ",/O\n",
      "不断/O\n",
      "接触到/VP\n",
      "的/O\n",
      "不同/O\n",
      "人群/NP\n",
      ",/O\n",
      "也/O\n",
      "让/VP\n",
      "我/O\n",
      "积累/VP\n",
      "了/O\n",
      "良好/O\n",
      "的/O\n",
      "社会关系/NP\n",
      ",/O\n",
      "现场/NP\n",
      "的/O\n",
      "活/VP\n",
      "\n",
      "动/VP\n",
      "执行/NP\n",
      ",/O\n",
      "更/O\n",
      "让/VP\n",
      "我/O\n",
      "在/PP\n",
      "面对/VP\n",
      "突发事件/NP\n",
      "时/O\n",
      ",/O\n",
      "有/VP\n",
      "了/O\n",
      "冷静/O\n",
      "思考的能力/NP\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Tokenizer = JClass('com.hankcs.hanlp.tokenizer.StandardTokenizer')\n",
    "\n",
    "huan_hang = set(['。', '？', '！', '?'])\n",
    "keep_pos = \"q,qg,qt,qv,s,t,tg,g,gb,gbc,gc,gg,gm,gp,mg,Mg,n,an,ude1,nr,ns,nt,\\\n",
    "            nz,nb,nba,nbc,nbp,nf,ng,nh,nhd,o,nz,nx,ntu,nts,nto,nth,ntch,ntcf,\\\n",
    "            ntcb,ntc,nt,nsf,ns,nrj,nrf,nr2,nr1,nr,nnt,nnd,nn,nmc,nm,nl,nit,\\\n",
    "            nis,nic,ni,nhm,nhd\"\n",
    "\n",
    "keep_pos_nouns = set(keep_pos.split(\",\"))\n",
    "keep_pos_v = \"v,vd,vg,vf,vl,vshi,vyou,vx,vi,vn\"\n",
    "keep_pos_v = set(keep_pos_v.split(\",\"))\n",
    "keep_pos_p = set(['p', 'pbei', 'pba'])\n",
    "merge_pos = keep_pos_p | keep_pos_v  # union\n",
    "keep_flag = set([\n",
    "    '：', '，', '？', '。', '！', '；', '、', '-', '.', '!', ',', ':', ';', '?', '(',\n",
    "    ')', '（', '）', '<', '>', '《', '》'\n",
    "])\n",
    "drop_pos_set = set([\n",
    "    'xu', 'xx', 'y', 'yg', 'wh', 'wky', 'wkz', 'wp', 'ws', 'wyy', 'wyz', 'wb',\n",
    "    'u', 'ud', 'ude1', 'ude2', 'ude3', 'udeng', 'udh'\n",
    "])\n",
    "\n",
    "\n",
    "def to_string(sentence, return_generator=False):\n",
    "    \"\"\"hanlp的sentence分词\"\"\"\n",
    "    if return_generator:\n",
    "        return (word_pos_item.toString().split('/')\n",
    "                for word_pos_item in Tokenizer.segment(sentence))\n",
    "    else:\n",
    "        return [(word_pos_item.toString().split('/')[0],\n",
    "                 word_pos_item.toString().split('/')[1])\n",
    "                for word_pos_item in Tokenizer.segment(sentence)]\n",
    "\n",
    "\n",
    "def cut_hanlp(raw_sentence, return_list=True):\n",
    "    \"\"\"分词结果返回list还是生成器（return_list=False）\"\"\"\n",
    "    if len(raw_sentence.strip()) > 0:\n",
    "        return to_string(raw_sentence) if return_list else iter(\n",
    "            to_string(raw_sentence))\n",
    "\n",
    "\n",
    "def getNodes(parent, model_tagged_file):\n",
    "    \"\"\"合并输入tree对象的目标子节点，输出名词短语块\"\"\"\n",
    "    text = ''\n",
    "    for node in parent:\n",
    "        if type(node) is nltk.Tree:\n",
    "            # 合并子节点的子节点：('文字'，'词性')\n",
    "            if node.label() == 'NP':\n",
    "                text += ''.join(\n",
    "                    node_child[0].strip()\n",
    "                    for node_child in node.leaves()) + \"/NP\" + \"\\n\"\n",
    "            if node.label() == 'VP':\n",
    "                text += ''.join(\n",
    "                    node_child[0].strip()\n",
    "                    for node_child in node.leaves()) + \"/VP\" + \"\\n\"\n",
    "        # 叶子节点\n",
    "        else:\n",
    "            # 介词保留\n",
    "            if node[1] in keep_pos_p:\n",
    "                text += node[0].strip() + \"/PP\" + \"\\n\"\n",
    "            # 符号不需要\n",
    "            if node[0] in huan_hang:\n",
    "                text += node[0].strip() + \"/O\" + \"\\n\"\n",
    "            # 其他不需要\n",
    "            if node[1] not in merge_pos:\n",
    "                text += node[0].strip() + \"/O\" + \"\\n\"\n",
    "    \n",
    "    print(text)\n",
    "    model_tagged_file.write(text + \"\\n\")\n",
    "\n",
    "\n",
    "def grammer(sentence, model_tagged_file):\n",
    "    \"\"\"\n",
    "    input sentences shape:[('工作', 'vn'), ('描述', 'v'), ('：', 'w'), \n",
    "                          ('我', 'rr'), ('曾', 'd'), ('在', 'p')]\n",
    "    \"\"\"\n",
    "    # 解析的词性和格式：<一个单元>   \n",
    "    #                                 |：  或者   \n",
    "    #                                 *：0或多次   \n",
    "    #                                +：1或者多次\n",
    "    #                               ？：0或者1次  \n",
    "    #                               { }：一条规则单元\n",
    "    # 词性按hanlp标准\n",
    "    grammar1 = r\"\"\"NP: \n",
    "        {<m|mg|Mg|mq|q|qg|qt|qv|s|>*<a|an|ag>*<s|g|gb|gbc|gc|gg|gm|gp|n|an|nr|ns|nt|nz|nb|nba|nbc|nbp|nf|ng|nh|nhd|o|nz|nx|ntu|nts|nto|nth|ntch|ntcf|ntcb|ntc|nt|nsf|ns|nrj|nrf|nr2|nr1|nr|nnt|nnd|nn|nmc|nm|nl|nit|nis|nic|ni|nhm|nhd>+<f>?<ude1>?<g|gb|gbc|gc|gg|gm|gp|n|an|nr|ns|nt|nz|nb|nba|nbc|nbp|nf|ng|nh|nhd|o|nz|nx|ntu|nts|nto|nth|ntch|ntcf|ntcb|ntc|nt|nsf|ns|nrj|nrf|nr2|nr1|nr|nnt|nnd|nn|nmc|nm|nl|nit|nis|nic|ni|nhm|nhd>+}\n",
    "        {<n|an|nr|ns|nt|nz|nb|nba|nbc|nbp|nf|ng|nh|nhd|nz|nx|ntu|nts|nto|nth|ntch|ntcf|ntcb|ntc|nt|nsf|ns|nrj|nrf|nr2|nr1|nr|nnt|nnd|nn|nmc|nm|nl|nit|nis|nic|ni|nhm|nhd>+<cc>+<n|an|nr|ns|nt|nz|nb|nba|nbc|nbp|nf|ng|nh|nhd|nz|nx|ntu|nts|nto|nth|ntch|ntcf|ntcb|ntc|nt|nsf|ns|nrj|nrf|nr2|nr1|nr|nnt|nnd|nn|nmc|nm|nl|nit|nis|nic|ni|nhm|nhd>+}\n",
    "        {<m|mg|Mg|mq|q|qg|qt|qv|s|>*<q|qg|qt|qv>*<f|b>*<vi|v|vn|vg|vd>+<ude1>+<n|an|nr|ns|nt|nz|nb|nba|nbc|nbp|nf|ng|nh|nhd|nz|nx|ntu|nts|nto|nth|ntch|ntcf|ntcb|ntc|nt|nsf|ns|nrj|nrf|nr2|nr1|nr|nnt|nnd|nn|nmc|nm|nl|nit|nis|nic|ni|nhm|nhd>+}\n",
    "        {<g|gb|gbc|gc|gg|gm|gp|n|an|nr|ns|nt|nz|nb|nba|nbc|nbp|nf|ng|nh|nhd|nz|nx|ntu|nts|nto|nth|ntch|ntcf|ntcb|ntc|nt|nsf|ns|nrj|nrf|nr2|nr1|nr|nnt|nnd|nn|nmc|nm|nl|nit|nis|nic|ni|nhm|nhd>+<vi>?}\n",
    "        VP:{<v|vd|vg|vf|vl|vshi|vyou|vx|vi|vn>+}\n",
    "        \"\"\"\n",
    "    # 解析器对象\n",
    "    cp = nltk.RegexpParser(grammar1)\n",
    "    try:\n",
    "        # 解析为tree，通过.draw()可视化\n",
    "        result = cp.parse(sentence)\n",
    "    except:\n",
    "        pass\n",
    "    else:\n",
    "        getNodes(result, model_tagged_file)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"主函数\"\"\"\n",
    "    fout = open('nvp.txt', 'w', encoding='utf8')\n",
    "\n",
    "    for line in open('text_2.txt', 'r', encoding='utf8'):\n",
    "        line = line.strip()\n",
    "        grammer(cut_hanlp(line), fout)\n",
    "\n",
    "    fout.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
