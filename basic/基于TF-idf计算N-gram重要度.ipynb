{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基于频率统计的N-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-29T14:21:28.371780Z",
     "start_time": "2020-02-29T14:21:28.366810Z"
    }
   },
   "outputs": [],
   "source": [
    "# %load tokenizer.py\n",
    "import os,gc,re,sys\n",
    "from itertools import chain\n",
    "from pyhanlp import *\n",
    "\n",
    "\n",
    "drop_pos_set=set(['xu','xx','y','yg','wh','wky','wkz','wp','ws','wyy','wyz','wb','u','ud','ude1','ude2','ude3','udeng','udh'])\n",
    "\n",
    "Tokenizer = JClass('com.hankcs.hanlp.tokenizer.StandardTokenizer')\n",
    "HanLP = JClass('com.hankcs.hanlp.HanLP')\n",
    "\n",
    "\n",
    "def to_string(sentence,return_generator=False):\n",
    "    if return_generator:\n",
    "        return (word_pos_item.toString().split('/') for word_pos_item in Tokenizer.segment(sentence))\n",
    "    else:\n",
    "        return [(word_pos_item.toString().split('/')[0],word_pos_item.toString().split('/')[1]) for word_pos_item in Tokenizer.segment(sentence)]   \n",
    "\n",
    "\n",
    "def to_string_hanlp(sentence,return_generator=False):\n",
    "    if return_generator:\n",
    "        return (word_pos_item.toString().split('/') for word_pos_item in HanLP.segment(sentence))\n",
    "    else:\n",
    "        return [(word_pos_item.toString().split('/')[0],word_pos_item.toString().split('/')[1]) for word_pos_item in Tokenizer.segment(sentence)]      \n",
    "\n",
    "\n",
    "def seg_sentences(sentence,with_filter=True,return_generator=False):  \n",
    "    segs = to_string(sentence,return_generator=return_generator)\n",
    "    if with_filter:\n",
    "        g = [word_pos_pair[0] for word_pos_pair in segs if len(word_pos_pair)==2 and word_pos_pair[0]!=' ' and word_pos_pair[1] not in drop_pos_set]\n",
    "    else:\n",
    "        g = [word_pos_pair[0] for word_pos_pair in segs if len(word_pos_pair)==2 and word_pos_pair[0]!=' ']\n",
    "    return iter(g) if return_generator else g\n",
    "\n",
    "\n",
    "def cut_hanlp(raw_sentence,return_list=True):\n",
    "    if len(raw_sentence.strip())>0:\n",
    "        return to_string(raw_sentence) if return_list else iter(to_string(raw_sentence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json,re\n",
    "import numpy\n",
    "import itertools\n",
    "from itertools import chain\n",
    "from tokenizer import seg_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 筛选出非字母和汉字的符号\n",
    "pattern=re.compile(u'[^a-zA-Z\\u4E00-\\u9FA5]')\n",
    "\n",
    "\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    \"\"\"Json处理numpy对象\"\"\"\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, numpy.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, numpy.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, numpy.ndarray):\n",
    "            return obj.tolist()        \n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "\n",
    "\n",
    "def generate_ngram(sentence, n=4, m=2):\n",
    "    \"\"\"生成ngram\n",
    "    n: N-Gram最大的n\n",
    "    m：N-Gram最小的n\n",
    "    \"\"\"\n",
    "    if len(sentence) < n:\n",
    "        n = len(sentence)\n",
    "    \n",
    "    temp = [\n",
    "        tuple(sentence[i - k:i]) for k in range(m, n + 1)\n",
    "        for i in range(k, len(sentence) + 1)\n",
    "    ]\n",
    "    \n",
    "    # 正则过滤掉含有标点符号item\n",
    "    return [\n",
    "        item for item in temp if len(''.join(item).strip()) > 1\n",
    "        and len(pattern.findall(''.join(item).strip())) == 0\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # 按字为单位\n",
    "    # \"RESUMEDOCSSTARTFLAG\" 在text文档中表示文件开始\n",
    "    copus_character = [\n",
    "        generate_ngram(line.strip())\n",
    "        for line in open('text.txt', 'r', encoding='utf8')\n",
    "        if len(line.strip()) > 0 and \"RESUMEDOCSSTARTFLAG\" not in line\n",
    "    ]\n",
    "    \n",
    "    # 按词为单位：优势在于实际语义保留更完整准确（而不是按字生成N-Gram）\n",
    "    # 分词后的结果进行统计N-gram\n",
    "    copus_word = [\n",
    "        generate_ngram(seg_sentences(line.strip(), with_filter=True))\n",
    "        for line in open('text.txt', 'r', encoding='utf8')\n",
    "        if len(line.strip()) > 0 and \"RESUMEDOCSSTARTFLAG\" not in line\n",
    "    ]\n",
    "    # 提高效率\n",
    "    copus_word = chain.from_iterable(copus_word)\n",
    "    copus_word = ['_'.join(item) for item in copus_word]\n",
    "    \n",
    "    # N-gram频次\n",
    "    fout = open(\"ngram2_4.txt\", \"w\", encoding='utf-8')\n",
    "    dic_filter = {}\n",
    "    for item in copus_word:\n",
    "        if item in dic_filter:\n",
    "            dic_filter[item] += 1\n",
    "        else:\n",
    "            dic_filter[item] = 1\n",
    "    sort_dic = sorted(dic_filter.items(), key=lambda val: val[1], reverse=True)\n",
    "    \n",
    "    fout.write(json.dumps(sort_dic, ensure_ascii=False, cls=NumpyEncoder))\n",
    "    fout.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基于TF-idf计算N-gram重要度\n",
    "\n",
    "说明：因以下算法文件读取方式，建议将输入文件的格式预处理为：一行一篇文档。\n",
    "\n",
    "一行表示一篇文档。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer  \n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from tokenizer import seg_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _replace_c(text):\n",
    "    intab = \",?!\"\n",
    "    outtab = \"，？！\"\n",
    "    deltab = \")(+_-.>< \"\n",
    "    trantab = text.maketrans(intab, outtab, deltab)\n",
    "    return text.translate(trantab)\n",
    "\n",
    "\n",
    "def tokenize_raw(text):\n",
    "    \"\"\"将原文档按标点符号分割\"\"\"\n",
    "    split_sen = (i.strip()\n",
    "                 for i in re.split('。|,|，|：|:|？|！|\\t|\\n', _replace_c(text))\n",
    "                 if len(i.strip()) > 5)\n",
    "    return [seg_sentences(sentence) for sentence in split_sen]\n",
    "\n",
    "\n",
    "def list_2_ngram(sentence, n=4, m=2):\n",
    "    \"\"\"生成ngram\n",
    "    n: N-Gram最大的n\n",
    "    m：N-Gram最小的n\n",
    "    \"\"\"\n",
    "    if len(sentence) < n:\n",
    "        n = len(sentence)\n",
    "    temp = [\n",
    "        tuple(sentence[i - k:i]) for k in range(m, n + 1)\n",
    "        for i in range(k,\n",
    "                       len(sentence) + 1)\n",
    "    ]\n",
    "    \n",
    "    return [\n",
    "        item for item in temp if len(''.join(item).strip()) > 1\n",
    "        and len(pattern.findall(''.join(item).strip())) == 0\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    copus = [\n",
    "        tokenize_raw(line.strip())\n",
    "        for line in open('text.txt', 'r', encoding='utf8')\n",
    "        if len(line.strip()) > 0 and \"RESUMEDOCSSTARTFLAG\" not in line\n",
    "    ]\n",
    "    \n",
    "    doc = []\n",
    "    if len(copus) > 1:\n",
    "        for list_copus in copus:\n",
    "            for t in list_copus:\n",
    "                doc.extend([\n",
    "                    ' '.join(['_'.join(i) for i in list_2_ngram(t, n=4, m=2)])\n",
    "                ])\n",
    "    doc = list(filter(None, doc))\n",
    "    \n",
    "    fout = open(\"ngram2_4_tfidf.txt\", \"w\", encoding='utf-8')\n",
    "    \n",
    "    vectorizer1 = CountVectorizer()   # 向量化统计词频\n",
    "    transformer = TfidfTransformer()  # 该类会统计每个词语的tf-idf权值\n",
    "    \n",
    "    freq1 = vectorizer1.fit_transform(doc)   # shape：（doc，word）\n",
    "    tfidf = transformer.fit_transform(freq1)\n",
    "    \n",
    "    # 每个word的词频在每个doc的值的和（word在此处就是n-gram）\n",
    "    word_freq = [freq1.getcol(i).sum() for i in range(freq1.shape[1])]\n",
    "    \n",
    "    # 每个word的tf-idf在每个doc的值的和（word在此处就是n-gram）\n",
    "    tfidf_sum = [tfidf.getcol(i).sum() for i in range(tfidf.shape[1])]\n",
    "    \n",
    "    tfidf_dic = vectorizer1.vocabulary_   # 字典化\n",
    "    tfidf_dic = dict(zip(tfidf_dic.values(), tfidf_dic.keys())) \n",
    "    \n",
    "    dic_filter = {}\n",
    "    def _add(wq, tf, i):\n",
    "        \"\"\"保存｛n-gram：（词频，tfidf）｝\n",
    "        tfidf在此处准确来讲是该词在各个文档中的tfidf之和\n",
    "        \"\"\"\n",
    "        dic_filter[tfidf_dic[i]] = [wq, tf]\n",
    "    \n",
    "    # 统计每个word全局的word_freq，tfidf_sum\n",
    "    for i, (word_freq_one, w_one) in enumerate(zip(word_freq, tfidf_sum)):\n",
    "        _add(word_freq_one, w_one, i)\n",
    "    \n",
    "    sort_dic = sorted(dic_filter.items(), key=lambda val: val[1],\n",
    "               reverse=True)\n",
    "\n",
    "    fout.write(json.dumps(sort_dic, ensure_ascii=False, cls=NumpyEncoder))\n",
    "    fout.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
